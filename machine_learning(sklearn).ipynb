{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    - 概述\n",
    "        - 机器学习的问题\n",
    "            - 建模问题\n",
    "                - 所谓机器学习，在形式上可这样理解：在数据对象中通过统计或推理的方法，寻找一个接受特定输入X，并给出预期输出Y的功能函数f，即Y=f(X)。\n",
    "            - 评估问题\n",
    "                - 针对已知的输入，函数给出的输出(预测值)与实际输出(目标值)之间存在一定的误差，因此需要构建一个评估体系，根据误差的大小判定函数的优劣。\n",
    "            - 优化问题\n",
    "                - 学习的核心在于改善性能，通过数据对算法的反复锤炼，不断提升函数预测的准确性，直至获得能够满足实际需求的最优解，这个过程就是机器学习。\n",
    "        - 机器学习的种类\n",
    "            - 监督学习、无监督学习、半监督学习、强化学习\n",
    "                - 有监督学习：用已知输出评估模型的性能。\n",
    "                - 无监督学习：在没有已知输出的情况下，仅仅根据输入信息的相关性，进行类别的划分。\n",
    "                - 半监督学习：先通过无监督学习划分类别，再根据人工标记通过有监督学习预测输出。\n",
    "                - 强化学习：通过对不同决策结果的奖励和惩罚，使机器学习系统在经过足够长时间的训练以后，越来越倾向于给出接近期望结果的输出。\n",
    "\n",
    "            - 批量学习和增量学习\n",
    "                - 批量学习：将学习的过程和应用的过程截然分开，用全部的训练数据训练模型，然后再在应用场景中实现预测，当预测结果不够理想时，\n",
    "                  重新回到学习过程，如此循环。\n",
    "                - 增量学习：将学习的过程和应用的过程统一起来，在应用的同时以增量的方式，不断学习新的内容，边训练边预测。\n",
    "        - 机器学习的一般过程\n",
    "            - 数据处理\n",
    "                - 数据收集 （数据检索、数据挖掘、爬虫）\n",
    "                - 数据清洗\n",
    "                - 特征工程\n",
    "            - 机器学习\n",
    "                - 选择模型 （算法）\n",
    "                - 训练模型 （算法）\n",
    "                - 评估模型 （工具、框架、算法知识）\n",
    "                - 测试模型\n",
    "            - 业务运维\n",
    "                - 应用模型\n",
    "                - 维护模型\n",
    "        - 机器学习的典型应用\n",
    "            - 股价预测、推荐引擎、自然语言识别、语音识别、图像识别、人脸识别\n",
    "        - 机器学习的基本问题\n",
    "            - 回归问题：根据已知的输入和输出寻找某种性能最佳的模型，将未知输出的输入代入模型，得到连续的输出。\n",
    "            - 分类问题：根据已知的输入和输出寻找某种性能最佳的模型，将未知输出的输入代入模型，得到离散的输出。\n",
    "            - 聚类问题：根据已知输入的相似程度，将其划分为不同的群落。\n",
    "            - 降维问题：在性能损失尽可能小的前提下，降低数据的复杂度。\n",
    "    - 数据预处理(数据预处理的过程： 输入数据 -> 模型 -> 输出数据)\n",
    "        - 均值移除(标准化)\n",
    "        - 范围缩放\n",
    "        - 归一化\n",
    "        - 二值化\n",
    "        - 独热编码\n",
    "        - 标签编码\n",
    "    - 线性回归(基于模型的算法)\n",
    "        - 评估训练结果误差（metrics）\n",
    "        - 模型的保存和加载\n",
    "        - 岭回归\n",
    "        - 多项式回归\n",
    "    - 决策树(基于实例的算法)\n",
    "    - 人工分类\n",
    "    - 逻辑分类\n",
    "    - 朴素贝叶斯分类\n",
    "    - 决策树分类\n",
    "\"\"\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    均值移除(标准化)\n",
    "        - 由于一个样本的不同特征值差异较大，不利于使用现有机器学习算法进行样本处理。均值移除可以让样本矩阵中的每一列的平均值为0，标准差为1。\n",
    "            - 如何使样本矩阵中的每一列的平均值为0呢？\n",
    "                例如有一列特征值表示年龄： 17, 20, 23\n",
    "                mean = (17 + 20 + 23)/3 = 20\n",
    "                a' = -3\n",
    "                b' =  0\n",
    "                c' =  3\n",
    "                完成！\n",
    "            - 如何使样本矩阵中的每一列的标准差为1呢？\n",
    "                a' = -3\n",
    "                b' =  0\n",
    "                c' =  3\n",
    "                s' = std(a', b', c')\n",
    "                [a'/s',  b'/s',  c'/s']\n",
    "        - 均值移除API\n",
    "            from sklearn import preprocessing as pp\n",
    "            A = pp.scale(array)\n",
    "                - scale:                用于对函数进行预处理，实现均值移除。\n",
    "                - array:                为原数组，返回A为均值移除后的结果。\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "# 数据预处理相关库\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "raw_samples = np.array([\n",
    "    [17., 100., 4000.],\n",
    "    [20., 80., 5000.],\n",
    "    [23., 75., 5500.]\n",
    "])\n",
    "\n",
    "std_samples = pp.scale(raw_samples)\n",
    "print(std_samples)\n",
    "print(std_samples.mean(axis=0))\n",
    "print(std_samples.std(axis=0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    范围缩放(等比例)\n",
    "        - 将样本矩阵中的每一列的最小值和最大值设定为相同的区间，统一各列特征值的范围。一般情况下会把特征值缩放至[0, 1]区间。\n",
    "            - 如何使一组特征值的最小值为0呢？\n",
    "                - 例如有一列特征值表示年龄： [17, 20, 23]\n",
    "                - 每个元素减去特征值数组所有元素的最小值即可：[0, 3, 6]\n",
    "            - 如何使一组特征值的最大值为1呢？\n",
    "                - [0, 3, 6]\n",
    "                - 把特征值数组的每个元素除以最大值即可：[0, 1/2, 1]\n",
    "        - 范围缩放API：\n",
    "            from sklearn import preprocessing as pp\n",
    "            - mms = pp.MinMaxScaler(feature_range=(0,1))\n",
    "                - feature_range                 缩放范围\n",
    "            - mms.fit_transform(array)\n",
    "                - array                         原始样本矩阵\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "raw_samples = np.array([\n",
    "    [17., 100., 4000.],\n",
    "    [20., 80., 5000.],\n",
    "    [23., 75., 5500.]\n",
    "])\n",
    "# 创建MinMax缩放器\n",
    "mms = pp.MinMaxScaler(feature_range=(0, 1))\n",
    "# 调用mms对象的方法执行缩放操作, 返回缩放过后的结果\n",
    "result = mms.fit_transform(raw_samples)\n",
    "print(result)\n",
    "\n",
    "print(\"*\" * 50)\n",
    "#案例（求取直线 y = kx + b 中的 k 与 b,并通过直接方程求取对应的 y）\n",
    "mms_simple = raw_samples.copy()\n",
    "#拿到原始数据的每一列的数据\n",
    "for col in mms_simple.T:\n",
    "    #拿到每一列的最小值\n",
    "    col_min = col.min()\n",
    "    #拿到每一列的最大值\n",
    "    col_max = col.max()\n",
    "    a = np.array([\n",
    "        [col_min, 1],\n",
    "        [col_max, 1]])\n",
    "    b = np.array([0, 1])\n",
    "    #获取未知数x的值\n",
    "    x = np.linalg.solve(a, b)\n",
    "    # print(x)\n",
    "    #求取每一个kx，并将结果赋值给col变量\n",
    "    col *= x[0]\n",
    "    #将col与b相加,并将结果赋值给col变量\n",
    "    col += x[1]\n",
    "#返回kx+b所求得的所有结果\n",
    "print(mms_simple)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    归一化\n",
    "        - 有些情况每个样本的每个特征值具体的值并不重要，但是每个样本特征值的占比更加重要。\n",
    "                            Python        Java         PHP\n",
    "                    2017      10           20           5\n",
    "                    2018      8            5            0\n",
    "          所以归一化即是用每个样本的每个特征值除以该样本各个特征值绝对值的总和。变换后的样本矩阵，每个样本的特征值绝对值之和为1。\n",
    "        - 归一化相关API：\n",
    "            from sklearn import preprocessing as pp\n",
    "            - pp.normalize(array,norm=\"l1\")\n",
    "                - array                     原始样本矩阵\n",
    "                - norm                      范数\n",
    "                    - l1                    l1范数，向量中个元素绝对值之和\n",
    "                    - l2                    l2范数，向量中个元素平方之和\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "raw_samples = np.array([\n",
    "    [17., 100., 4000.],\n",
    "    [20., 80., 5000.],\n",
    "    [23., 75., 5500.]\n",
    "])\n",
    "# 归一化预处理\n",
    "result = pp.normalize(raw_samples, norm=\"l1\")\n",
    "print(result)\n",
    "print(\"*\" * 50)\n",
    "#手动实现归一化\n",
    "nor_samples = raw_samples.copy()\n",
    "for row in nor_samples:\n",
    "    row /= np.sum(np.abs(row))\n",
    "print(nor_samples)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    二值化\n",
    "        - 有些业务并不需要分析矩阵的详细完整数据（比如图像边缘识别只需要分析出图像边缘即可），可以根据一个事先给定的阈值，用0和1表示特征值不高于\n",
    "          或高于阈值。二值化后的数组中每个元素非0即1，达到简化数学模型的目的。\n",
    "        - 二值化相关API：\n",
    "            from sklearn import preprocessing as pp\n",
    "                - bin = pp.Binarizer(threshold)\n",
    "                    - threshold                  阈值\n",
    "                - result = bin.transform(array)\n",
    "                    - array                      原始样本矩阵\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "raw_samples = np.array([\n",
    "    [17., 100., 4000.],\n",
    "    [20., 80., 5000.],\n",
    "    [23., 75., 5500.]\n",
    "])\n",
    "# 根据给定的阈值创建一个二值化器\n",
    "bin_simples = pp.Binarizer(threshold=80)\n",
    "# 通过二值化器进行二值化预处理\n",
    "result = bin_simples.transform(raw_samples)\n",
    "print(result)\n",
    "print(\"*\" * 50)\n",
    "#手动实现二值化\n",
    "binar_samples = raw_samples.copy()\n",
    "binar_samples[binar_samples <= 80] = 0\n",
    "binar_samples[binar_samples > 80] = 1\n",
    "print(binar_samples)\n",
    "\n",
    "#二值化图片\n",
    "from scipy import ndimage\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#True:提取灰度图片\n",
    "from matplotlib import rc\n",
    "\n",
    "rc(\"font\", family='Microsoft YaHei')\n",
    "original = ndimage.imread(\"1.jpg\", True)\n",
    "bin_images = pp.Binarizer(threshold=127)\n",
    "result = bin_images.transform(original)\n",
    "plt.imshow(result, cmap=\"gray\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    独热编码(使用场景:适用于离散型的文本 如:计算文本相似度)\n",
    "        - 为样本特征的每个值建立一个由一个1和若干个0组成的序列，用该序列对所有的特征值进行编码。\n",
    "            两个数   三个数\t四个数\n",
    "              1\t\t 3\t\t 2\n",
    "              7\t\t 5\t\t 4\n",
    "              1\t\t 8\t\t 6\n",
    "              7\t\t 3\t\t 9\n",
    "            为每一个数字进行独热编码：\n",
    "            1-10    3-100\t2-1000\n",
    "            7-01    5-010   4-0100\n",
    "                    8-001   6-0010\n",
    "                            9-0001\n",
    "            编码完毕后得到最终经过独热编码后的样本矩阵：\n",
    "            101001000\n",
    "            010100100\n",
    "            100010010\n",
    "            011000001\n",
    "        - 独热编码相关API：\n",
    "            from sklearn import preprocessing as pp\n",
    "            - ohe = sp.OneHotEncoder(sparse,dtype)\n",
    "                - sparse                       是否使用紧缩格式（稀疏矩阵）\n",
    "                - dtype                        数据类型\n",
    "            - ohe_dict = ohe.fit(array)\n",
    "                - array                        原始样本矩阵\n",
    "            - ohe_samples = ohe_dict.transform(array)\n",
    "                - array                        原始样本矩阵\n",
    "\n",
    "\"\"\"\n",
    "from sklearn import preprocessing as pp\n",
    "import numpy as np\n",
    "\n",
    "raw_samples = np.array([\n",
    "    [17., 100., 4000],\n",
    "    [20., 80., 5000],\n",
    "    [23., 75., 5500]])\n",
    "\n",
    "# 创建独热编码器\n",
    "ohe = pp.OneHotEncoder(sparse=False, dtype=int)\n",
    "# 用独特编码器对原始样本矩阵做独热编码\n",
    "# ohe_dict = ohe.fit(raw_samples)\n",
    "# ohe_samples = ohe_dict.transform(raw_samples)\n",
    "#或者\n",
    "ohe_samples = ohe.fit_transform(raw_samples)\n",
    "print(ohe_samples)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    标签编码\n",
    "        根据字符串形式的特征值在特征序列中的位置，为其指定一个数字标签，用于提供给基于数值算法的学习模型。\n",
    "        - 标签编码相关API：\n",
    "            - lbe = sp.LabelEncoder()\n",
    "            - lbe_samples = lbe.fit_transform(array)\n",
    "                - array                        原始样本矩阵\n",
    "            - origin_samples = lbe.inverse_transform(array)\n",
    "                - array                        标签编码结果\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "raw_samples = np.array([\n",
    "    'audi', 'ford', 'audi', 'toyota',\n",
    "    'ford', 'bmw', 'toyota', 'ford',\n",
    "    'audi'])\n",
    "print(raw_samples)\n",
    "# 获取标签编码器\n",
    "lbe = pp.LabelEncoder()\n",
    "# 调用标签编码器的fit_transform方法训练并且为原始样本矩阵进行标签编码\n",
    "lbe_samples = lbe.fit_transform(raw_samples)\n",
    "print(lbe_samples)\n",
    "# 根据标签编码的结果矩阵反查字典 得到原始数据矩阵\n",
    "samples = lbe.inverse_transform(lbe_samples)\n",
    "print(samples)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    线性回归(概念及手动实现)\n",
    "        输入\t\t 输出\n",
    "        0.5      5.0\n",
    "        0.6      5.5\n",
    "        0.8      6.0\n",
    "        1.1      6.8\n",
    "        1.4      7.0\n",
    "        ...\n",
    "        y = f(x)\n",
    "        - 预测函数：y = w₀+w₁x\n",
    "            x: 输入\n",
    "            y: 输出\n",
    "            w₀和w₁: 模型参数\n",
    "        - 所谓模型训练，就是根据已知的x和y，找到最佳的模型参数w₀和w₁，尽可能精确地描述出输入和输出的关系\n",
    "            5.0 = w₀+ w₁ * 0.5\n",
    "            5.5 = w₀+ w₁ * 0.6\n",
    "        - 单样本误差：\n",
    "            根据预测函数求出输入为x时的预测值：y' = w₀+ w₁x，单样本误差为1/2(y' - y)²。\n",
    "        - 总样本误差：\n",
    "            把所有单样本误差相加即是总样本误差：1/2*∑(y' - y)²\n",
    "        - 损失函数：\n",
    "            loss = 1/2*∑(w₀+ w₁x - y)²\n",
    "            - 所以损失函数就是总样本误差关于模型参数的函数，该函数属于三维数学模型，即需要找到一组w₀、w₁使得loss取极小值。\n",
    "\n",
    "\"\"\"\n",
    "from sklearn import linear_model as lm\n",
    "#案例：画图模拟梯度下降的过程\n",
    "#1. 整理训练集数据，自定义梯度下降算法规则，求出w₀, w₁，绘制回归线。\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.mplot3d as axes3d\n",
    "\n",
    "# from matplotlib import rc\n",
    "# rc(\"font\",family='Microsoft YaHei')\n",
    "\n",
    "train_x = np.array([0.5, 0.6, 0.8, 1.1, 1.4])\n",
    "train_y = np.array([5.0, 5.5, 6.0, 6.8, 7.0])\n",
    "test_x = np.array([0.45, 0.55, 1.0, 1.3, 1.5])\n",
    "test_y = np.array([4.8, 5.3, 6.4, 6.9, 7.3])\n",
    "#设置w0,w1初始值\n",
    "w0, w1 = [1], [1]  #使用列表是为了绘制变化曲线，在该图中，三维曲面图中x轴为w₀, y轴为w₁\n",
    "#迭代次数(需要自己调参，根据loss判断,如果loss在最后两次迭代中相差足够小，即可)\n",
    "times = 1000\n",
    "#学习率(需要自己调参)\n",
    "lrate = 0.01\n",
    "#损失函数列表(绘制变化曲线使用)\n",
    "losses = []\n",
    "#迭代次数列表(绘制变化曲线使用)\n",
    "epoches = []\n",
    "for i in range(1, times + 1):\n",
    "    epoches.append(i)\n",
    "    #求损失函数关于w0与w1的偏导数,从而更新模型参数  损失函数公式：1/2*∑(w₀+ w₁x - y)²\n",
    "    #损失函数对w0求偏导数 d0 = ∑(w₀+ w₁x - y)\n",
    "    d0 = (w0[-1] + w1[-1] * train_x - train_y).sum()\n",
    "    #损失函数对w1求偏导数 d1 = ∑[x(w₀+ w₁x - y)]\n",
    "    d1 = (train_x * (w0[-1] + w1[-1] * train_x - train_y)).sum()\n",
    "    #根据梯度下降公式,更新w₀,w₁模型参数(lrate * 偏导 :每次梯度下降的距离)\n",
    "    w0_distance = w0[-1] - lrate * d0\n",
    "    w1_distance = w1[-1] - lrate * d1\n",
    "    w0.append(w0_distance)\n",
    "    w1.append(w1_distance)\n",
    "    loss = ((w0[-1] + w1[-1] * train_x - train_y) ** 2).sum() / 2\n",
    "    losses.append(loss)\n",
    "    # print(\"迭代次数:{0:4} w₀:{1:.8f} w₁:{2:.8f} loss:{3:.8f}\".format(i,w0_distance,w1_distance,loss))\n",
    "\n",
    "# print(len(epoches),len(w0),len(w1),len(losses))\n",
    "\n",
    "#求出最佳的模型参数w₀,w₁\n",
    "# print(\"最佳的模型参数w₀:{},w₁:{}\".format(w0,w1))\n",
    "\n",
    "\n",
    "#根据训练好的w₀和w₁，预测test测试数组，并获得测试数组y的值\n",
    "pre_test_y = w0[-1] + w1[-1] * test_x\n",
    "plt.figure(\"Linear Regression\", facecolor=\"lightgray\")\n",
    "plt.title(\"Linear Regression\", fontdict={\"fontsize\": 18})\n",
    "plt.grid(linestyle=\":\")\n",
    "#画散点图\n",
    "plt.scatter(train_x, train_y, s=80, marker=\"o\", color=\"dodgerblue\", label=\"Train Samples\")\n",
    "plt.scatter(test_x, test_y, s=80, marker=\"o\", color=\"green\", label=\"Test Samples Point\")\n",
    "plt.scatter(test_x, pre_test_y, s=80, marker=\"o\", color=\"purple\", label=\"Predict Test Samples Point\")\n",
    "plt.plot(test_x, pre_test_y, '--', c='limegreen', label='Predict Test Samples Regression', linewidth=1)\n",
    "\n",
    "#通过最佳的模型参数w₀,w₁,绘制回归线\n",
    "linex = np.linspace(train_x.min(), train_x.max(), 100)\n",
    "liney = w0[-1] + w1[-1] * linex  # y = w₀+w₁x\n",
    "\n",
    "plt.plot(linex, liney, color=\"orangered\", linewidth=2, label=\"Regression Line\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "#绘制w₀,w₁,loss的变化曲线\n",
    "plt.figure(\"Training Progress\", facecolor=\"lightgray\")\n",
    "plt.subplot(311)\n",
    "plt.grid(linestyle=\":\")\n",
    "plt.xlabel(\"iterations\", fontdict={\"fontsize\": 16})\n",
    "plt.ylabel(\"$w_0$\", fontdict={\"fontsize\": 16})\n",
    "plt.plot(epoches, w0[:-1], color=\"dodgerblue\", label=\"w₀\")\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.grid(linestyle=\":\")\n",
    "plt.xlabel(\"iterations\", fontdict={\"fontsize\": 16})\n",
    "plt.ylabel(\"$w_1$\", fontdict={\"fontsize\": 16})\n",
    "plt.plot(epoches, w1[:-1], color=\"dodgerblue\", label=\"w₁\")\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.grid(linestyle=\":\")\n",
    "plt.xlabel(\"iterations\", fontdict={\"fontsize\": 16})\n",
    "plt.ylabel(\"loss\", fontdict={\"fontsize\": 16})\n",
    "plt.plot(epoches, losses, color=\"orangered\", label=\"loss\")\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# 三维梯度下降图(训练过程图)\n",
    "number = 500\n",
    "grid_w0, grid_w1 = np.meshgrid(np.linspace(0, 9, number), np.linspace(0, 3.5, number))\n",
    "grid_loss = np.zeros_like(grid_w0)\n",
    "\n",
    "for x, y in zip(train_x, train_y):\n",
    "    grid_loss += ((grid_w0 + x * grid_w1 - y) ** 2) / 2\n",
    "\n",
    "plt.figure(\"Loss Function\")\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "plt.title(\"Loss Function\", fontdict={\"fontsize\": 20})\n",
    "ax.set_xlabel(\"$w_0$\", fontdict={\"fontsize\": 14})\n",
    "ax.set_ylabel(\"$w_1$\", fontdict={\"fontsize\": 14})\n",
    "ax.set_zlabel(\"loss\", fontdict={\"fontsize\": 14})\n",
    "#绘制三维曲面图(指定w₀,w₁的范围，计算loss值)\n",
    "ax.plot_surface(grid_w0, grid_w1, grid_loss, rstride=10, cstride=10, cmap=\"jet\")\n",
    "#绘制梯度下降曲线\n",
    "ax.plot(w0[:-1], w1[:-1], losses, 'o-', c='red', label='BGD')\n",
    "\n",
    "#绘制等高线图\n",
    "plt.figure('Batch Gradient Descent', facecolor='lightgray')\n",
    "plt.title('Batch Gradient Descent', fontsize=20)\n",
    "plt.xlabel('x', fontsize=14)\n",
    "plt.ylabel('y', fontsize=14)\n",
    "plt.tick_params(labelsize=10)\n",
    "plt.grid(linestyle=\":\")\n",
    "#绘制等高线图(指定w₀,w₁的范围，计算loss值)\n",
    "cutr = plt.contour(grid_w0, grid_w1, grid_loss, 10, colors=\"black\", linewidths=0.5)\n",
    "#绘制梯度下降曲线\n",
    "plt.clabel(cutr, inline_spacing=0.1, fmt=\"%.2f\", fontsize=8)\n",
    "plt.contourf(grid_w0, grid_w1, grid_loss, 10, cmap=\"jet\")\n",
    "plt.plot(w0, w1, 'o-', c='orangered', label='BGD')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    线性回归(sklearn实现)\n",
    "         - 操作步骤\n",
    "            - 采集数据(读文本)\n",
    "            - 整理输入集(二维)与输出集(一维)\n",
    "            - 构建线性回归模型,训练模型\n",
    "            - 针对训练数据,得到预测结果,画图\n",
    "\n",
    "        - 线性回归sklearn相关API：\n",
    "            import sklearn.linear_model as lm\n",
    "            - 创建模型\n",
    "                - model = lm.LinearRegression()\n",
    "                - model.fit(input, output)      通过梯度下降法计算模型参数(训练模型)\n",
    "                    - input             输入为一个二维数组表示的样本矩阵\n",
    "                    - output            输出为每个样本最终的结果(一维数组)\n",
    "                - model.predict(array)          预测输出\n",
    "                    - array             二维数组，每一行是一个样本，每一列是一个特征。\n",
    "\n",
    "        - 评估训练结果误差（metrics）\n",
    "            - 线性回归模型训练完毕后，可以利用测试集评估训练结果误差。sklearn.metrics提供了计算模型误差的几个常用算法：\n",
    "                - import sklearn.metrics as sm\n",
    "                    - sm.mean_absolute_error(y, predict_y)   平均绝对值误差：1/m∑|实际输出-预测输出|\n",
    "                        - y                     实际值\n",
    "                        - predict_y             预测值\n",
    "                    - sm.mean_squared_error(y, predict_y)    平均平方误差：SQRT(1/mΣ(实际输出-预测输出)^2)\n",
    "                        - y                     实际值\n",
    "                        - predict_y             预测值\n",
    "                    - sm.median_absolute_error(y, pred_y)    中位绝对值误差：MEDIAN(|实际输出-预测输出|)\n",
    "                        - y                     实际值\n",
    "                        - predict_y             预测值\n",
    "                    - sm.r2_score(y, pred_y)                 R2得分，(0,1]区间的分值。分数越高，误差越小。\n",
    "                        - y                     实际值\n",
    "                        - predict_y             预测值\n",
    "\n",
    "        - 模型的保存和加载\n",
    "            - 模型训练是一个耗时的过程，一个优秀的机器学习是非常宝贵的。可以模型保存到磁盘中，也可以在需要使用的时候从磁盘中重新加载模型即可。\n",
    "              不需要重新训练。\n",
    "                import pickle\n",
    "                - pickle.dump(内存对象, 磁盘文件)   保存模型\n",
    "                - model = pickle.load(磁盘文件)    加载模型\n",
    "\n",
    "        - 岭回归\n",
    "            - 普通线性回归模型使用基于梯度下降的最小二乘法，在最小化损失函数的前提下，寻找最优模型参数，于此过程中，包括少数异常样本在内的全\n",
    "              部训练数据都会对最终模型参数造成程度相等的影响，异常值对模型所带来影响无法在训练过程中被识别出来。为此，岭回归在模型迭代过程所\n",
    "              依据的损失函数中增加了正则项，以限制模型参数对异常样本的匹配程度，进而提高模型面对多数正常样本的拟合精度。\n",
    "            import sklearn.linear_model as lm\n",
    "                - model = lm.Ridge(alpha，fit_intercept, max_iter)\n",
    "                    - alpha                     正则强度(默认1.0)\n",
    "                    - fit_intercept             是否训练截距\n",
    "                    - max_iter                  最大迭代次数\n",
    "                - model.fit(input, output)      训练模型\n",
    "                    - input             输入为一个二维数组表示的样本矩阵\n",
    "                    - output            输出为每个样本最终的结果(一维数组)\n",
    "                - model.predict(array)          预测输出\n",
    "                    - array             二维数组，每一行是一个样本，每一列是一个特征。\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "import sklearn.linear_model as lm\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#采集数据\n",
    "x, y = np.loadtxt(\"./ml_data/single.txt\", delimiter=\",\", unpack=True, usecols=(0, 1))\n",
    "#将维度转为二维数据(其中每列为特征数据:sklearn规定)\n",
    "x = x.reshape(-1, 1)  #n行1列的二维数组(唯一特征列)\n",
    "\n",
    "#创建线性模型对象\n",
    "model = lm.LinearRegression()\n",
    "#训练模型\n",
    "model.fit(x, y)\n",
    "#根据输入预测输出\n",
    "predict_y = model.predict(x)\n",
    "# print(predict_y)\n",
    "plt.figure('Linear Regression', facecolor=\"lightgray\")\n",
    "plt.title('Linear Regression', fontsize=20)\n",
    "plt.xlabel(\"x\", fontdict={\"fontsize\": 14})\n",
    "plt.ylabel(\"y\", fontdict={\"fontsize\": 14})\n",
    "plt.tick_params(labelsize=10)\n",
    "plt.grid(linestyle=':')\n",
    "plt.scatter(x, y, c='dodgerblue', alpha=0.75, s=60, label='Sample')\n",
    "plt.plot(x, predict_y, c='orangered', label='Regression')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"*\" * 50)\n",
    "import sklearn.metrics as sm\n",
    "\n",
    "'''评估训练结果误差（metrics）'''\n",
    "# 平均绝对值误差：1/m∑|实际输出-预测输出|\n",
    "print(\"平均绝对值误差: {}\".format(sm.mean_absolute_error(y, predict_y)))\n",
    "# 平均平方误差：SQRT(1/mΣ(实际输出-预测输出)^2)\n",
    "print(\"平均平方误差: {}\".format(sm.mean_squared_error(y, predict_y)))\n",
    "# 中位绝对值误差：MEDIAN(|实际输出-预测输出|)\n",
    "print(\"中位绝对值误差: {}\".format(sm.median_absolute_error(y, predict_y)))\n",
    "# R2得分，(0,1]区间的分值。分数越高，误差越小。\n",
    "print(\"R2得分: {}\".format(sm.r2_score(y, predict_y)))\n",
    "\n",
    "print(\"*\" * 50)\n",
    "'''模型的保存和加载'''\n",
    "import pickle\n",
    "\n",
    "with open(\"sklearn_data/linear.pkl\", \"wb\") as f:\n",
    "    #保存模型对象\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "with open(\"sklearn_data/linear.pkl\", \"rb\") as f:\n",
    "    #加载模型对象\n",
    "    model = pickle.load(f)\n",
    "    predict_y = model.predict(x)\n",
    "    print(predict_y)\n",
    "\n",
    "print(\"*\" * 50)\n",
    "#岭回归(特殊的线性回归,弱化异常值对整体样本的影响,使拟合直线更加匹配正常样本)\n",
    "\n",
    "x, y = np.loadtxt(\"./ml_data/abnormal.txt\", delimiter=\",\", unpack=True, usecols=(0, 1))\n",
    "x = x.reshape(-1, 1)\n",
    "#使用线性回归\n",
    "model = lm.LinearRegression()\n",
    "\n",
    "model.fit(x, y)\n",
    "predict_y1 = model.predict(x)\n",
    "#使用岭回归\n",
    "model = lm.Ridge(210, fit_intercept=True, max_iter=10000)  #若alpha为0，则为线性回归\n",
    "model.fit(x, y)\n",
    "predict_y2 = model.predict(x)\n",
    "\n",
    "plt.figure(\"Linear Regression & Ridge Regression\", facecolor=\"lightgray\")\n",
    "plt.title(\"Linear Regression & Ridge Regression\", fontdict={\"fontsize\": 20})\n",
    "plt.grid(linestyle=\":\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "#绘制原始样本点分布\n",
    "plt.scatter(x, y, s=30, c=\"dodgerblue\", marker=\"o\", label=\"Simple Point\")\n",
    "plt.plot(x, predict_y1, c=\"orangered\", label=\"Linear Regression Line\")\n",
    "plt.plot(x, predict_y2, c=\"purple\", label=\"Ridge Regression Line\")\n",
    "#绘制异常点指向\n",
    "plt.annotate(\"Abnormal Point\", xycoords=\"data\", xy=(5, 11), textcoords=\"offset points\",\n",
    "             xytext=(20, -50), fontsize=14, arrowprops=dict(\n",
    "        arrowstyle=\"->\",\n",
    "        connectionstyle=\"angle3\")\n",
    "             )\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#计算线性回归及岭回归的得分(得分的高低根据测试样本来定，而不是下面的训练样本)\n",
    "#在正常样本中(不含异常值)，岭回归得分会比线性回归得分高\n",
    "print(\"线性回归R2得分: {} 岭回归R2得分: {}\".format(sm.r2_score(y, predict_y1), sm.r2_score(y, predict_y2)))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    多项式回归(基于模型的算法)\n",
    "        - 若希望回归模型更好的拟合训练样本数据，可以使用多项式回归器。\n",
    "            - 一元多项式回归\n",
    "                y = w₀ + w₁x + w₂x² + w₃x³ + ... + wₙxⁿ\n",
    "                将高次项看做对一次项特征的扩展得到：\n",
    "                y = w₀ + w₁x₁ + w₂x₂ + w₃x₃ + ... + wₙxₙ\n",
    "                那么一元多项式回归即可以看做为多元线性回归，可以使用LinearRegression模型对样本数据进行模型训练。\n",
    "            - 一元多项式回归的实现需要两个步骤：\n",
    "                1. 将一元多项式回归问题转换为多元线性回归问题（只需给出多项式最高次数即可）。\n",
    "                2. 将1步骤得到多项式的结果中 w₁   w₂ .. 当做样本特征，交给线性回归器训练多元线性模型。\n",
    "            - 使用sklearn提供的数据管线实现两个步骤的顺序执行：\n",
    "                import sklearn.pipeline as pl\n",
    "                import sklearn.preprocessing as sp\n",
    "                import sklearn.linear_model as lm\n",
    "                - model = pl.make_pipeline(sp.PolynomialFeatures(10),lm.LinearRegression())\n",
    "                    - sp.PolynomialFeatures(degree)           多项式特征扩展器(一元多项式回归方程转化为多元线性回归方程)\n",
    "                        - degree                                    最高次数\n",
    "                    - lm.LinearRegression()                   线性回归器\n",
    "\n",
    "            - 欠拟合\n",
    "                - 过于简单的模型，无论对于训练数据还是测试数据都无法给出足够高的预测精度，这种现象叫做欠拟合。\n",
    "            - 过拟合\n",
    "                - 过于复杂的模型，对于训练数据可以得到较高的预测精度，但对于测试数据通常精度较低，这种现象叫做过拟合。\n",
    "            - 一个性能可以接受的学习模型应该对训练数据和测试数据都有接近的预测精度，而且精度不能太低。\n",
    "                           训练集R2   测试集R2\n",
    "                            0.3        0.4        欠拟合：过于简单，无法反映数据的规则\n",
    "                            0.9        0.2        过拟合：过于复杂，太特殊，缺乏一般性\n",
    "                            0.7        0.6        可接受：复杂度适中，既反映数据的规则，同时又不失一般性\n",
    "\"\"\"\n",
    "import sklearn.pipeline as pl\n",
    "import sklearn.preprocessing as sp\n",
    "import sklearn.linear_model as lm\n",
    "import sklearn.metrics as sm\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#案例：\n",
    "#采集数据\n",
    "x, y = np.loadtxt(\"./ml_data/single.txt\", delimiter=\",\", unpack=True, usecols=(0, 1))\n",
    "x = x.reshape(-1, 1)\n",
    "model = pl.make_pipeline(sp.PolynomialFeatures(10),  # 多项式特征扩展器\n",
    "                         lm.LinearRegression())  # 线性回归器\n",
    "\n",
    "model.fit(x, y)\n",
    "pre_y = model.predict(x)\n",
    "#绘制多项式函数图像，从min到max拆500个点\n",
    "#预测500个函数值，按顺序连线。\n",
    "test_x = np.linspace(x.min(), x.max(), 1000).reshape(-1, 1)\n",
    "pre_test_y = model.predict(test_x)\n",
    "#画图\n",
    "plt.figure(\"Polynomial Regression\", facecolor=\"lightgray\")\n",
    "plt.title(\"Polynomial Regression\", fontdict={\"fontsize\": 20})\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.grid(linestyle=\":\")\n",
    "plt.scatter(x, y, c=\"dodgerblue\", marker=\"o\", s=50, label=\"Sample Point\")\n",
    "\n",
    "plt.plot(x, pre_y, color=\"purple\", alpha=0.3, label=\"Train1 Polynomial Regression Line\")\n",
    "#Train1 Polynomial Regression Line曲线比较混乱,故采用本图代替\n",
    "plt.plot(test_x, pre_test_y, color=\"orangered\", alpha=0.7, label=\"Train2 Polynomial Regression Line\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"R2得分: {}\".format(sm.r2_score(y, pre_y)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2得分: 0.8202560889408635\n",
      "2.7670975940796905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PycharmProjects\\dataAnalysis\\venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    决策树(基于实例的算法)：图在sklearn_data文件中\n",
    "        - 基本算法原理\n",
    "            - 核心思想：相似的输入必会产生相似的输出。例如预测某人薪资：\n",
    "                年龄：1-青年，2-中年，3-老年\n",
    "                学历：1-本科，2-硕士，3-博士\n",
    "                经历：1-出道，2-一般，3-老手，4-骨灰\n",
    "                性别：1-男性，2-女性\n",
    "\n",
    "                年龄    学历    经历    性别    ==>    薪资\n",
    "                 1       1      1      1     ==>    6000（低）\n",
    "                 2       1      3      1     ==>    10000（中）\n",
    "                 3       3      4      1     ==>    50000（高）\n",
    "                ...     ...    ...    ...    ==>     ...\n",
    "                 1       3      2      2     ==>      ?\n",
    "                为了提高搜索效率，使用树形数据结构处理样本数据：\n",
    "                             |⁻ 学历1              |⁻ 学历1                  |⁻ 学历1\n",
    "                    年龄 = 1 <   学历2    年龄 = 2 <   学历2        年龄 = 3 <   学历2\n",
    "                             |₋ 学历3              |₋ 学历3                  |₋ 学历3\n",
    "                首先从训练样本矩阵中选择第一个特征进行子表划分，使每个子表中该特征的值全部相同，然后再在每个子表中选择下一个特征按照同样\n",
    "                的规则继续划分更小的子表，不断重复直到所有的特征全部使用完为止，此时便得到叶级子表，其中所有样本的特征值全部相同。\n",
    "                对于待预测样本，根据其每一个特征的值，选择对应的子表，逐一匹配，直到找到与之完全匹配的叶级子表，用该子表中样本的输出，\n",
    "                通过平均(回归)或者投票(分类)为待预测样本提供输出。\n",
    "                随着子表的划分，信息熵（信息的混乱程度）越来越小，信息越来越纯，数据越来越有序。\n",
    "            - 决策树回归器模型相关API：\n",
    "                import sklearn.tree as st\n",
    "                - 创建决策树回归器模型  决策树的最大深度为4\n",
    "                    - model = st.DecisionTreeRegressor(max_depth=4)\n",
    "                -训练模型\n",
    "                    - model.fit(train_x, train_y)\n",
    "                        - train_x           二维数组样本数据\n",
    "                        - train_y           训练集中对应每行样本的结果\n",
    "                - 测试模型\n",
    "                        - pred_test_y = model.predict(test_x)\n",
    "        - 工程优化\n",
    "            - 不必用尽所有的特征，叶级子表中允许混杂不同的特征值，以此降低决策树的层数，在精度牺牲可接受的前提下，提高模型的性能。通常情况下，\n",
    "              可以优先选择使 *信息熵 减少量最大的特征作为划分子表的依据。\n",
    "        - 集合算法\n",
    "            - 根据多个不同模型给出的预测结果，利用平均(回归)或者投票(分类)的方法，得出最终预测结果。\n",
    "                - 基于决策树的集合算法，就是按照某种规则，构建多棵彼此不同的决策树模型，分别给出针对未知样本的预测结果，最后通过平均或投票得\n",
    "                  到相对综合的结论。\n",
    "            - 正向激励\n",
    "                - 首先为样本矩阵中的样本随机分配初始权重，由此构建一棵带有权重的决策树，在由该决策树提供预测输出时，通过加权平均或者加权投票的方\n",
    "                  式产生预测值。将训练样本代入模型，预测其输出，对那些预测值与实际值不同的样本，提高其权重，由此形成第二棵决策树。重复以上过程，\n",
    "                  构建出不同权重的若干棵决策树。\n",
    "                - 正向激励相关API：\n",
    "                    import sklearn.tree as st\n",
    "                    import sklearn.ensemble as se\n",
    "                    - model = st.DecisionTreeRegressor(max_depth=4)\n",
    "                        - max_depth                      最大深度\n",
    "                    - model = se.AdaBoostRegressor(model, n_estimators=400, random_state=7)    自适应增强决策树回归模型\n",
    "                        - model                          决策树模型（一颗）\n",
    "                        - n_estimators                   构建400棵不同权重的决策树，训练模型\n",
    "                        - random_state                   随机种子(增强迭代使用)\n",
    "                    - model.fit(train_x, train_y)           训练模型\n",
    "                    - pred_test_y = model.predict(test_x)   预测模型\n",
    "                - 特征重要性\n",
    "                    - 作为决策树模型训练过程的副产品，根据每个特征划分子表前后的信息熵减少量就标志了该特征的重要程度，此即为该特征重要性指标。\n",
    "                      训练得到的模型对象提供了属性：feature_importances_来存储每个特征的重要性。\n",
    "                    - 样本矩阵特征重要性属性\n",
    "                        - model.fit(train_x, train_y)\n",
    "                        - fi = model.feature_importances_\n",
    "            - 自助聚合\n",
    "                - 每次从总样本矩阵中以有放回抽样的方式随机抽取部分样本构建决策树，这样形成多棵包含不同训练样本的决策树，以削弱某些强势样本对\n",
    "                  模型预测结果的影响，提高模型的泛化特性。\n",
    "            - 随机森林（属于集合算法的一种）\n",
    "                - 在自助聚合的基础上，每次构建决策树模型时，不仅随机选择部分样本，而且还随机选择部分特征，这样的集合算法，不仅规避了强势样本\n",
    "                  对预测结果的影响，而且也削弱了强势特征的影响，使模型的预测能力更加泛化。\n",
    "                - 随机森林相关API：\n",
    "                    import sklearn.ensemble as se\n",
    "                    - model = se.RandomForestRegressor(max_depth,n_estimators,min_samples_split)     随机森林回归模型\n",
    "                        - max_depth                         决策树最大深度10\n",
    "                        - n_estimators                      构建1000棵决策树，训练模型\n",
    "                        - min_samples_split                 子表中最小样本数 若小于这个数字，则不再继续向下拆分\n",
    "\n",
    "\"\"\"\n",
    "# 案例：预测波士顿地区房屋价格。\n",
    "import sklearn.datasets as sd\n",
    "# 打乱数据集用\n",
    "import sklearn.utils as su\n",
    "import sklearn.ensemble as se\n",
    "\n",
    "# 获得波士顿地区房屋价格的第一种方式，注意：load_boston()函数在1.2版本中已经被分离\n",
    "boston = sd.load_boston()\n",
    "\n",
    "# 获得波士顿地区房屋价格的第二种方式\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "# raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "# data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "# target = raw_df.values[1::2, 2]\n",
    "\n",
    "# print(boston.data.shape)   # 输入数据\n",
    "# print(boston.target.shape) # 输出数据\n",
    "# print(boston.feature_names) # 输入数据的特征名称\n",
    "\n",
    "# 打乱原始数据集的输入和输出\n",
    "# random_state : 随机种子\n",
    "# 使用相同的随机种子多次打乱得到的结果是一致的\n",
    "x, y = su.shuffle(boston.data, boston.target, random_state=7)\n",
    "# 划分训练集和测试集\n",
    "train_size = int(len(x) * 0.8)  #从总样本中挑出80%用作训练集，剩下20%用作测试集\n",
    "train_x, train_y, test_x, test_y = x[:train_size], y[:train_size], x[train_size:], y[train_size:]\n",
    "# print(train_x,train_y)\n",
    "# print(test_x,test_y)\n",
    "# 构建决策树模型\n",
    "import sklearn.tree as st\n",
    "import sklearn.metrics as sm\n",
    "# 创建决策树回归模型\n",
    "model = st.DecisionTreeRegressor(max_depth=4)\n",
    "#训练模型\n",
    "model.fit(train_x,train_y)\n",
    "#预测模型\n",
    "pred_test_y = model.predict(test_x)\n",
    "print(\"R2得分: {}\".format(sm.r2_score(test_y,pred_test_y)))\n",
    "print(sm.mean_absolute_error(test_y,pred_test_y))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yes', 'yes', 'no', 'no', 'no']\n",
      "[['1' '1' 'yes']\n",
      " ['1' '1' 'yes']\n",
      " ['1' '0' 'no']\n",
      " ['0' '1' 'no']\n",
      " ['0' '1' 'no']] <class 'numpy.ndarray'> ['no surfacing', 'flippers'] <class 'list'>\n",
      "['yes', 'yes', 'no']\n",
      "[['1' 'yes']\n",
      " ['1' 'yes']\n",
      " ['0' 'no']] <class 'numpy.ndarray'> ['flippers'] <class 'list'>\n",
      "['yes', 'yes']\n",
      "['no']\n",
      "['no', 'no']\n",
      "{'no surfacing': {'1': {'flippers': {'1': 'yes', '0': 'no'}}, '0': 'no'}}\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAADxCAYAAABoIWSWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA3U0lEQVR4nO3dd1QUZ//+8TcdARGxI7awgooRS+wmFuwaS9QHY4vmsSeWRGOLVDX2x5IYJfZuxBZ7xMYvFuyiAgprBYkiIILiAgvz+8NvODEmURSYBT6vczzRZXfm2iVe3N4zc4+RoigKQggh8oSx2gGEEKIwkdIVQog8JKUrsiU5OZmmTZvi5ubG7du3qV+/PvXr1+f27dvUqlWLDz/8kFu3blGtWjVat26NVqvFycmJzp07ExERQcWKFenZsydhYWE4ODjQr18/rly5QqlSpRgyZAgXLlzA3t6e0aNHc/r0aWxtbZk0aRJBQUHY2Ngwbdo0fv31V4oUKcL8+fP55ZdfsLS05Mcff2TLli1YWFiwZs0aVq9ejYWFBVu2bGHp0qVYWlryyy+/MH/+fIoUKcKvv/7KtGnTsLGxISgoiEmTJmFra8vp06cZM2YMxYsX58KFCwwZMoRSpUpx5coV+vXrh4ODA2FhYfTs2ZOKFSty69Yttb8lIr9RhMiGTp06KV27dlX69++vmJqaKh4eHoqHh4diamqqDBgwQOnWrZtiamqqDBkyROnQoYNiamqqfPHFF0qrVq0UMzMz5euvv1aaNm2qmJubKxMmTFAaNGigWFhYKN9++61Su3ZtxdLSUvH29lZcXV0VKysrxc/PT6latapStGhRZcaMGUqVKlWUYsWKKTNnzlQcHR0Ve3t7ZdasWUq5cuWUkiVLKrNmzVLKlCmjlClTRpk1a5ZSsmRJpVy5csqsWbMUe3t7pUKFCsrMmTOVYsWKKVWqVFFmzJihFC1aVKlatari5+enWFlZKa6uroq3t7diaWmp1K5dW/n2228VCwsLpUGDBsrEiRMVc3NzpWnTpsrYsWMVR0dHRa/Xq/1tEfmIkaLIgTTx5r755hsOHz7M999/z927d3FxcUFRFCIiInBxcSEzMxOtVouLiwt6vZ5bt27h7OxMeno6d+/eRaPRkJqaSnR0NE5OTuh0Oh48eEDlypV59uwZcXFxVKpUiadPn/L48WMqVKhAUlISSUlJODo6kpiYSEpKCg4ODjx+/JjU1FTKli1LfHw8GRkZlC5dmri4OABKlixJbGwsJiYmlChRggcPHmBhYUHx4sWJiYnBysoKOzs7oqOjsbW1xdbWlqioKIoXL46NjQ13796lVKlSWFlZcefOHcqWLYulpSW3bt3CwcGB+fPnk5SUxKFDhzA2ln80ijcjpSuyRa/X4+zsTL9+/ejatavacVQTGRnJgAEDuHPnDg4ODmrHEfmI/HgW2TJu3DhKlChBu3bt1I4CwKVLl/jPf/5Dnz590Ol02Xrttm3b2Lt371vtV6PR0L17dzw8PMjMzHyrbYjCyVTtACJ/uXXrFpUrV8bCwkLtKGRkZHDgwAEGDhxIx44ds/36nj17vvW+jYyMcHZ25tSpU2RmZsr0gnhjMr0gsuXJkye4ubkxfPhw2rRpQ0xMDKNHj6Z27dpZZyHMnz8fS0tLbty4wcyZM9HpdDg6OuLl5YWtre1L2zt8+DA//fQTJiYm2NjYsHz5cvbs2UNYWBgTJ04EYOzYsfTr148PPviADz/8kE8++YSzZ8/i7u7Opk2bsLGxoVatWkyZMoVx48aRlJSEXq9nxIgRtGjRAoC9e/eyYcMGjIyM0Gg0TJs2DX9/f6ysrOjfvz9Dhw6lZs2anD9/nqdPn+Lp6UmdOnXQ6XT4+Phw8+ZNKlWqxKNHj5g4cSLW1tb069ePS5cu4eLiktffBpGPyUhXZMvu3btJTU2lZs2aWY9FRUUxY8YMpk6dyqRJkzh69CgdO3bE29ubb775hnr16rFs2TKWL1/OuHHjXtre8uXL+eGHHyhdujTJycmv3f/z58+pWbMmX331FQDR0dE0a9aM1q1bo9frmTt3LjY2NiQmJjJw4ECaN2/OrVu3WLVqFatWrcLOzo4nT5787bYzMjJYt24dJ06cYPny5fz4448EBARQtGhRAgIC0Gq19O3bF4AyZcrg4uLC0qVLWbBgAUZGRm/7kYpCRv5NJLJl0aJF9OzZk3LlymU95uDgkDXaq1atGjExMTx9+pTk5GTq1asHQOfOnbl48eIr23Nzc8PHx4edO3eSkZHx2v2bmJjQqlWrf/z6kiVL6N27NyNHjuTRo0fEx8dz7tw53N3dsbOzA6BYsWJ/+9qWLVsCUL16dWJiYgC4fPly1vy1RqNBo9EAYGlpyeDBg1m2bBnp6emvzS3EH6R0RbasXbuWn3/+mcuXL2c9ZmZmlvV7ExOTNyrPP0yZMoWRI0fy8OFD+vfvT2JiIiYmJvx51istLS3r9+bm5kRGRqLX6zl//vxLFyccOHCAxMRENmzYwKZNm7C3t3/pta9jbm7+xu8hLi4OX19ffv7556zXCfEmpHRFtuj1+jc6cGRjY4OtrS2XLl0CYN++fdStW/eV50VHR1OzZk2GDx9O8eLFefjwIQ4ODkRERJCZmcmDBw8IDQ3Ner6iKIwcOZLnz59TpUoVoqKiOHToEIqi8PTpU4oXL46pqSnnz5/n999/B6B+/focOXKExMREgH+cXvg7bm5uBAYGAi8OImq1WuDFgTRjY2NSU1PfeFtCgMzpimwaNGgQffr0oVatWq99ro+PT9aBtPLly+Pt7f3KcxYtWsS9e/dQFIUGDRrg7OwMvJiy6NWrF1WqVHnpQFV6ejo9evSgaNGiADRq1IirV6+yePFiBgwYwNdff42Hhwc1atSgcuXKADg5OfH5558zdOhQTExMcHFxwcfH543eb69evfD29qZXr15UrlwZJycnbGxsKFGiBN7e3vTr149u3brJaFe8MTl7QWTL1q1bGT16NP7+/jg6Oubpvh88eEDfvn0JCAjA3t4+6/HExES+/PJL6tSpw9dff52jB7UyMjLQ6/VYWFgQHR3NyJEj2b59O+np6YwcOZJOnToxa9asHNufKPhkekFkS5s2bbCwsODGjRt5vu+VK1fyySefvFS4AHZ2dixdupSrV68ya9asHL1YQafTMXjwYD799FPGjx/PxIkTMTMzIz4+nps3b9K/f/8c25coHKR0Rbb06dOHxo0b07BhQ1avXk1SUhJPnjxh9erVPH36lISEBNasWcPz58959OgRa9euJTU1ld9//53169eTnp5OVFQUGzduRK/Xc/v2bbZs2UJmZiaRkZFs27YNRVEIDw9n586dKIrC1atXWbt2LUePHsXNzY0DBw4AcO7cuaz51mvXruHh4cHNmzcZNWoUQUFBABw7dozTp08DEBgYyLlz54AXB90uXbqEoijs2bOHq1evoigKO3fuJDw8HEVR2LZtGzExMaxdu5auXbsyc+ZMGjZsyMaNGwGYMGECbdq0Qa/X5/W3QeRjMr0gsuXbb79lx44dWFpaZpWNoigYGRlhamrKs2fPsLCwoEiRIsTFxWFtbZ21kEyxYsWwt7cnMjISe3t7SpcuTVhYGPb29pQvX57Lly9TokQJKleuzPnz5ylevDhVq1blzJkzpKWl4eDgQHx8PDY2Nri5uXHy5EmsrKyoW7cup06dwszMjDp16nDo0CFMTExo06YNFy5cID09nSZNmnDhwgWeP39O06ZNCQkJITk5mSZNmhAWFsaTJ09o0KABkZGRJCYmUq9ePW7fvk1CQgK1a9fm/v37JCQkUKNGDWJjY0lISMDZ2ZkiRYqwf/9+OU9XvDEZ6YpsmT59Op9++ilt27YlJCSEbt268cknnxASEkKbNm347LPPuHz5Mk2aNOGLL77g4sWL1K5dmwkTJnD+/HmcnZ3x9fXl3LlzVKpUiTlz5nD27FnKlCnDkiVLOHPmDHZ2dqxatYrg4GCsra2ZPXs25ubmODs7s337dk6cOAG8uMosKCiIzMxMDh48yLFjxzAyMuLkyZPUqVOHCxcucOjQIQ4ePEhmZia//fYbe/fuzXrOjh07MDMz49SpU2zcuBFra2uCg4NZuXIldnZ2nD17liVLllC2bFnOnj3LnDlzqFSpEufOncPX1xeNRsOuXbukcEW2yEhXGLzevXvj5ubG5MmT3/g1Op2OHj16ZC1kLmcXCEMhpSsM2pUrV2jbti1arRYbG5tsvTY1NRUPDw8yMjIICAjA0tIyl1IK8eZkekEYNG9vbyZMmJDtwgWwsLAgICCAIkWK0K1bN54/f54LCYXIHhnpCoN14cIFunTpglarpUiRIm+9Hb1ez2effcaDBw/YvXs31tbWOZhSiOyRka4wWJ6enkyZMuWdChfA1NSUdevWUbFiRTp06PBGq5kJkVukdIVBOn36NKGhoQwePDhHtmdiYsLKlSupXr067dq1y9b6C0LkJCldYZA8PT3x9PTM0TtUGBsbs2zZMurVq0fr1q15/Phxjm1biDclpSsMzvHjx7lz5w6fffZZjm/byMiIxYsX89FHH9GqVausOwcLkVekdIVBURQFT09PvL29X1qnNycZGRkxb948OnToQMuWLXn48GGu7EeIvyNLOwqDEhgYSFxcHH369MnV/RgZGTFjxgwsLCxo0aIFR44ckVupizwhpSsMhqIoTJ06FV9fX0xMTHJ9f0ZGRnh7e2Nubk7z5s05evQoFSpUyPX9isJNSlcYjL1795KamvpOt0Z/G5MnT36peP9Y/FyI3CClKwxCZmYmnp6e+Pr6vvZWQLlh3LhxmJubZ001ODk55XkGUThI6QqDsGPHDkxNTenatatqGUaNGpU1x3v48OGXbhMkRE6R0hWqy8jIwNvbm3nz5qm+TOLQoUMxMzOjVatWHDp0CFdXV1XziIJHSleobsuWLdjZ2dG+fXu1owAvbr5pbm5O69atOXjwIG5ubmpHEgWIlK5QlV6vx8fHB39/f9VHuX/Wt29fzM3NadeuHfv37//b28cL8TakdIWq1q1bh6OjI61atVI7yit69eqFmZkZHTp0YPfu3TRs2FDtSKIAkKUdhWrS0tJwcXFh/fr1NGvWTO04/2jfvn0MGjSIHTt2GHROkT/IZcBCNatWrcLFxcXgi6xTp05s2LCBTz75hOPHj6sdR+RzMtIVqtDpdGg0Gnbu3En9+vXVjvNGjh07hoeHB5s2baJ169ZqxxH5lIx0hSr8/f2pV69evilcgJYtW7J9+3b69OnD/v371Y4j8ikZ6Yo89+zZMzQaTb49HSs4OJguXbqwfPlyVS/mEPmTnL0g8tySJUv48MMP82XhAjRq1IgDBw7QqVMn0tLS6NWrl9qRRD4iI12Rp5KSktBoNBw/fpwaNWqoHeedhISE0L59e+bPn5/rS1GKgkNGuiJPLVq0iHbt2uX7wgVwc3Pj8OHDtG3blvT09Fy504UoeKR0RZ55/PgxixYtIjg4WO0oOcbV1ZUjR47Qpk0b0tLSGDJkiNqRhIGT0hV55n//+x9du3ZFo9GoHSVHVatWjWPHjuHu7k5aWhpffPGF2pGEAZPSFXkiLi6OH3/8kQsXLqgdJVdoNBqCgoJo1aoVaWlpfPXVV2pHEgZKSlfkiTlz5uDh4VGg78pQuXLlrOJNTU1l0qRJakcSBkjOXhC57sGDB7i6unLlyhXKly+vdpxcFxMTQ6tWrejTpw+enp4GtXqaUJ+Ursh1Y8aMwdjYmAULFqgdJc88fPgQd3d3unbtyvTp06V4RRYpXZGroqKiqF27NmFhYZQpU0btOHnq0aNHtGnThtatWzN37lwpXgFI6YpcNnz4cOzs7Jg1a5baUVSRkJBA27ZtadKkCYsWLZLiFVK6Ivfcvn2b+vXrc+PGDUqUKKF2HNUkJibSoUMH3Nzc+PHHH1W527EwHPLdF7lm2rRpfPHFF4W6cAHs7Ow4dOgQoaGhDB48mIyMDLUjCRXJSFfkioiICJo2bUpkZCR2dnZqxzEIz5494+OPP8bBwYE1a9ZgaipnbBZGMtIVucLX15exY8dK4f6JtbU1e/fu5dGjR/Tt25f09HS1IwkVyEhX5Lhr167h7u6OVqulaNGiascxODqdjp49e2Jubs6WLVswNzdXO5LIQzLSFTnOx8eHb775Rgr3H1haWrJjxw4UReGTTz5Bp9OpHUnkIRnpihx16dIlOnXqhFarxcrKSu04Bi09PZ1+/fqRmJjIzp075fMqJGSkK97dn35ue3l5MXnyZCmQN2BmZsbGjRspXbo0nTt35unTpy++oNerG0zkKhnpineTmfniv8bGnDl9ml4eHkRERGBpaalurnwkIyODIUOGcCsigkN162JuZAQffwxyx+ECSUa64u2tXg2OjuDtDYCnlxdTp06Vws0mExMTVixfju/jxxzbtIlnrq4wezYsWQKpqWrHEzlMSle8nadP4ZdfYOJE2LeP3zZvRnvzJoPkljVvxfjZMz4qVozDPXvS/KefSB42DCIiICBA7Wgih0npirdjYwOLF8OYMSht2jB1zBi8vb0xs7BQO1n+ZGuLUeXKzKlRg5YtW9LW15ckjQZOnYIHD9ROJ3KQlK54exUrAnCkXj0eJCfTt3TpF4/LZa5vp3t3jEJCmPPVV7h37crghQt5lpEBv/+udjKRg6R0xTtRFAXPBQvw6dsX0z9WEjMxAbnaKvuaNYOSJTFau5bp06dT87PPuL5+PY/u3VM7mchBUrrinezfv5+nT5/i8dNPUKoUjBkDo0bBpUtqR8t/ypWDrl3hwAEICMBrwABKODgw/MsviYqKUjudyCFSuuKtKYqCl5cXvr6+GOt0EBsLmzZB1arQoIHa8fKnJk1g8uQXxdu+PZXHjqXZ11/TvHlzbt++rXY6kQNkmSPx1nbt2oWiKHTv3h3mz4e6dSEwEORg2rvp0OHFObpGRmBqyleAubk5LVq04MiRIwXuFvaFjVwcId5KZmYmbm5uzJo1i06dOr24SEIW585Vy5cvx8/Pj8DAQKpVq6Z2HPGWZKQr3srWrVuxtramY8eOLx6Qws11Q4YMwdzcHHd3d3799Vdq1qypdiTxFqR0Rbbp9Xq8vb1ZsmSJ3PMrj3322WeYmZnRpk0bDh48iJubm9qRRDZJ6Yps27hxI2XLlsXd3V3tKIVSnz59MDc3p23btuzbt48PPvhA7UgiG2ROV2RLeno6Li4urFmzho8++kjtOIXaL7/8wpAhQ9i9ezeNGjVSO454QzLSFdmyevVqNBqNFK4B6Nq1K2ZmZnTp0oUdO3bQrFkztSOJNyAjXfHGdDodzs7OBAQE0LBhQ7XjiP8TGBhInz592Lp1Ky1btlQ7jngNOeQs3tjy5ctxc3OTwjUwbdq0ISAgAA8PDw4dOqR2HPEaMtIVbyQlJQWNRsO+ffuoU6eO2nHE3zh58iTdu3dn9erVL86dFgZJRrrijSxdupTGjRtL4Rqwpk2bsmfPHj7//HN27typdhzxD2SkK14rOTkZjUbDkSNH5IT8fODixYt07NiRxYsX85///EftOOIv5OwF8Vrff/897u7uUrj5RN26dfn1119p37496enp9O3bV+1I4k+kdMW/SkxMZMGCBZw8eVLtKCIb3NzcOHz4MG3btiUtLY1BgwapHUn8Hyld8a8WLFhA586dcXZ2VjuKyCZXV1eOHj1K69atSUtLY9iwYWpHEkjpin8RHx/PDz/8wPnz59WOIt6Si4sLx48fx93dnbS0NEaNGqV2pEJPSlf8o7lz59KrVy+qVKmidhTxDpycnDh+/DitWrUiLS2NcePGqR2pUJOzF8TfevjwITVq1ODy5ctUqFBB7TgiB0RFReHu7s7AgQOZMmWK2nEKLRnpir81a9Ys+vXrJ4VbgFSoUIGgoKCsEa+3t7cszakCGemKV9y/f59atWoRGhpK2bJl1Y4jctjDhw9p3bo1nTt35rvvvpPizWNSuuIVI0eOxNramrlz56odReSSuLg42rRpQ6tWrZg3b54Ubx6S0hUvuXv3LnXr1uX69euUKlVK7TgiFz1+/Jh27drRsGFDFi1ahLHccilPSOmKlwwePJiyZcsyffp0taOIPPDkyRM6dOhAzZo1WbZsmRRvHpDSFVm0Wi2NGjUiMjKS4sWLqx1H5JHk5GQ6d+5MlSpVWLlyJSYmJmpHKtDkx5rI4uvry5gxY6RwC5miRYuyf/9+oqKiGDBgAHq9Xu1IBZqMdAUAYWFhtGjRAq1Wi62trdpxhAqeP39O9+7dKVq0KJs2bcLMzEztSAWSjHQFAD4+PowfP14KtxArUqQIu3btQqfT0atXL1JTU9WOVCDJSFcQEhJC+/bt0Wq1WFtbqx1HqCwtLY1PP/0UnU7H9u3bsbS0VDtSgSIjXYGXlxcTJ06UwhUAmJubs2XLFmxtbfn4449JSUlRO1KBIiPdQu7cuXN0794drVYrIxrxkoyMDAYNGkRUVBR79uzBxsZG7UgFgox0CzlPT0++/fZbKVzxChMTE1avXo2TkxPt27cnKSlJ7UgFgpRuIXby5Elu3LjBf//7X7WjCANlYmLCTz/9RK1atWjbti2JiYlqR8r3pHQLMU9PTzw9PTE3N1c7ijBgxsbGLFmyhEaNGuHu7k58fLzakfI1Kd1C6ujRo1knwwvxOkZGRixYsIDWrVvTqlUrYmNj1Y6Ub0npFjL169cnLS0NT09PfHx8MDWVJZXFmzEyMmLWrFl07dqVli1b8uDBA7Uj5UvyN66QuXLlCgcPHiQxMZEePXqQkpKClZWV2rFEPmFkZISfnx9mZma0aNGCI0eOUL58eQAURZElIt+AjHQLmYyMDHx9fRkyZAiNGjVi5cqVakcS+ZCnpyeff/45zZs35969e+zYsYORI0eqHStfkPN0CxkjIyMcHR3R6XTMmDGDIUOGyOhEvLWFCxeyaNEiNm3aRKdOnYiJiZHTD19DphcKkT9+vhoZGXHs2DFq1qypciKRn2VmZtK7d28sLCzw8PDA2dmZAwcO0L17d7WjGTSZXihEjIyMGDt2LOHh4VK44p3FxcVRt25dfvzxR2rXrs21a9fw9/dXO5bBk+kFIcRby8zMJDg4mB07drBu3Tri4uLQ6/VyB4p/IZ+MAVAUhaCgIHQ6HQDBwcFZV/5cunSJhw8fAhAeHs7du3cBuHXrFhEREQDExMRw5coVABISEjh79iwAz54947fffgMgPT2do0ePkpmZiaIoHD16lPT0dABOnDjB06dPATh//jxxcXEAXL16lfv37wMQGRnJrVu3cvVzEPmPsbExTZo0Yd68eTx8+BCtViuF+zqKUFVmZqYyefJkxcrKSmnVqpXy3XffKTY2Nkrt2rWVJUuWKLa2tkqVKlWU1atXK8WLF1fKlCmjrF+/XilVqpRib2+vrF27VqlQoYJSrFgxxd/fX6lRo4ZStGhRZd68eUqzZs0Ua2trZdKkSUq3bt0Ua2tr5fPPP1eGDRumWFtbK506dVK8vLwUa2trpVGjRsqCBQuUokWLKi4uLsqKFSsUOzs7pXz58sq6deuUEiVKKKVKlVIuXryo9kcm8oH4+HhlwYIFyrBhwxR3d3elSpUqiqmpqQK88qtkyZJK/fr1ld69eytTp05VTp8+rXb8XCXTCyoLDg6mZcuWbN++HX9/f8LDw1mwYAHbtm3j8OHDzJs3j3PnzrF27VpmzpzJo0ePmDdvHlOmTMHKyoqpU6cybNgwXF1dGT9+PF26dKFjx4589dVX1KtXj8GDBzN+/HjKli3LpEmTmDp1KhkZGcycOZO5c+dy7949FixYwOrVqwkODmbBggUcPnyY7du3M2fOHLRaLT/88AN+fn5ERERw9OhRrl27pvbHJgzYgwcPqFOnDh988AHVq1fH0dGR8uXL4+Dg8Mol54qiEB8fT3R0NNHR0URFRbFv3z68vb0ZPny4Su8gd0npqiw9PZ2uXbtibGyMr6+v2nH+0f379xk6dCiLFy/mP//5j9pxhAGbOnUqN2/eZPz48W/1eq1Wy5dffklsbGyBPJ1RJl9UZmZmRs+ePblw4YLaUf7VnTt3MDY2pnnz5mpHEQYuJCSE2rVrv/XrnZycSEtL49GjRzkXyoBI6aosNDSUr776ioULF2Y9tmXLFnr27EmHDh2YPXs2ANu2bWPv3r0qpYSmTZvSvn17evXqpVoGkT+Eh4dTpUqVt369kZERTk5OhIeH52AqwyGlq7IyZcpQsmRJLl26lPVYQEAAS5Yseemyyp49e9K5c+dcy6EoCpmZmf/49efPn3Pp0iUaNGiQaxlE/peWlkZ0dDQVK1Z8p+1UqlSpwJauXJGmspIlS7Jw4UI8PDzo1asX3333Hffv32f06NF06dIl63n+/v5YWVnRv39/hg4dirOzMxcvXkSv1+Pl5UXNmjXx9/fPOiCRmJjIgAEDsq4OWrduHYcPHyYtLY2WLVsybNgwYmJi+PLLL6lZsybXr19n0aJF+Pv7ExYWhpGREV26dKFv374AHD9+nLi4OL777jtVPieRPzx79gwLC4uXbt8eExPD6NGjqV27NleuXKFUqVLMnz+fu3fvMnPmTHQ6HY6Ojnh5eWXdjbpYsWI8efJErbeRq2Skq7KYmBiGDRuGp6cnAFOmTKFUqVL4+/v/6+3QdTodmzZtYtKkSfj5+WU9rtVqWbp0KatXr2bFihU8evSI4OBgoqKiWLt2LZs2bSI8PJyLFy8CEBUVRa9evdi6dSuJiYnExsaydetWfv7555dKv02bNlSuXFkWNRFv5c//nxUtWpSjR4/i7e3NqFGj2LJlCxqNhuXLl6sdM09I6ars2bNn6HQ6ypQpk63XtWvXDoC6devy7NkzkpOTAWjevDmWlpbY2dlRr149QkNDCQ4OJjg4mL59+9KvXz/u3LnDvXv3AChXrhzvv/8+AOXLl+f+/fvMmTOHU6dOvXR3YBMTE8qVK0dMTExOvG1RyDg4OODi4gJAtWrViI6OJjk5mXr16gHQuXPnrIFAQSfTCyqrWrUqS5cuZcSIEQQGBr7x6/56Ks0ffzYyMuLw4cNoNBqioqJ48uQJiqIwcOBAevTo8dJr/roilK2tLZs3b+b06dNs376dwMBAvL29ATh27BinT58mNDT0bd+qKMT+PN1gYmKSNUgojGSkq7KUlBQWLVpEx44ds/W6Q4cOAXD58mVsbGyybo999OhRZs6cSUpKCrdu3eLcuXM0btyY3bt3k5KSAkBsbCwJCQmvbDMxMZHMzEzc3d0ZMWIEN27cyPparVq1MDIyYuPGjW/7VkUhYGlpSWpqKnq9/l+fZ2Njg62tbdYB5H379lG3bt2srxfkxfVlpKuy0NBQQkJCskaUb8rCwoI+ffpkHUj7M2NjY7799luGDRvGypUrGT58OO3bt2fQoEEAWFlZMW3atFeukY+NjcXX1zdrCcgvvvgi62slS5akR48e+Pv7M2LEiLd5q6IQKFKkCGXKlCEmJua1ZzD4+PhkHUgrX778S38H7t69y+DBg3M7rirkijQD4Ofnx5YtW1i/fv0bPX/o0KGMHTuWGjVqvPT4woUL2bZtGwEBAZQrVw6An376ifv377/z1W4hISFMnDiRoKCgV/YrxJ+1bduW9u3bv9OFNO3bt+fixYtUqFAhB5MZBpleUJmiKMTGxr500OpthYSE4OzsnFW4AH369OHkyZPcuXPnnbZtYWFBZmYmSUlJ75hSFHSurq7vdI7tw4cPs04jK4ikdFUWHBzMypUrmTFjBuvWrWPy5Mk8e/aM3bt3M3bsWOLj4/ntt98YMWIEUVFRWUs4GhkZcfv2bYYPH05wcDDh4eGEh4fTvn17kpOTmTBhAps3b8bMzIyyZcvyzTffoNfrmT9/PrNnz0av17N8+XK8vLzQ6XQEBAQwfvx4njx5wuHDh/nyyy958OAB58+fZ9iwYZiZmTFgwIAC+08+kXOGDx/O9u3bWb16NWfOnCE6Ovpf53gVRSExMZFr165x8OBBJkyYwBdffFEg110AmV5QnV6vp3fv3pw7d44iRYrQoEEDgoKC0Ov1fPLJJ+zcuROdTsfnn3/OunXrSE9PZ9iwYfz000+YmJgwaNAgVqxYgU6nw8XFhZiYGMzNzWndujXHjx8nIyMDNzc39u/fT+nSpXF2dsbU1BStVoutrS3VqlXjzJkzwIvT0A4cOEBqair9+vVj8+bNpKWlMWTIEFauXImiKKxatYqPP/5Y5U9NGLrQ0FAWLlxIREQEt27dIjY2ltKlS//tKmOPHj3CxMSEKlWq8N5779G5c2c+++yzgrsub96vJin+Kj09Xfnxxx+Vhw8fKpmZmcrKlSuVmzdvKoqiKFu2bFFCQkIURVGUvXv3KidPnlQURVGCgoKUgwcPZj1uY2OjPHz4ULlx44aydu1aJTMzU7l//76ybNkyRa/XK9OnT1dq1aqlpKSkKM+fP1e+//57JSEhQdHr9Yq/v78SFRWlZGZmKuvXr1fCw8MVRVGUnTt3KufOnVMURVECAwOVoKCgvP5oRAGh0+kUrVarhIeHv/IrISFB7Xh5Ska6BcDQoUMpUaIEM2fO/MfnPH/+HI1Gw+7du7NOSBdC5D0p3Xzu5s2bNGzYkIiICOzt7f/1uUuWLGHfvn3s378/j9IJIf5KSjef++yzz6hSpQo+Pj6vfW5qairOzs5s3ryZJk2a5H44IcQrpHTzsevXr/Phhx+i1WopVqzYG71mxYoVbN68mSNHjuRyOiHE3ymghwcLB19fX77++us3Llx4MTK+e/cux48fz71gQoh/JCPdfOrq1au0adMGrVabte7Cm1q/fj0//fQT/+///b8Cey6kEIZKRrr5lLe3NxMmTMh24cKLq9Ti4uKyFs0RQuQdGenmQxcuXKBLly5otVqKFCnyVtv4+eefmT9/PmfOnJHRrhB5SEa6+ZCXlxdTpkx568IF6NWrFzqdTtWbXQpRGMlIN585ffo0vXv3JiIiAgsLi3fa1q5du/Dx8eHixYsF95JLIQyM/E3LZzw9PZk6deo7Fy5A165dMTU1ZceOHTmQTAjxJmSkm48EBQXx+eefc/369Zduf/IuDhw4wLhx47h69SomJiY5sk0hxD+TkW4+oSgKnp6eeHt751jhwovFou3s7NiyZUuObVMI8c+kdPOJwMBAYmNj6du3b45u18jIiOnTp+Pj4/Pa+1oJId6dlG4+8Mco19fXN1emAFq1aoWjoyPr1q3L8W0LIV4mpZsP7Nu3j+fPn9OrV69c28e0adOYNm0aaWlpubYPIYSUrsHLzMzE09MTPz+/XD2tq1mzZri4uLBq1apc24cQQkrX4O3cuRMTExO6du2a6/vy8/Nj+vTp6HS6XN+XEIWVlK4By8jIwMvLCz8/vzy5VLdBgwbUq1cPf3//XN+XEIWVnKdrwDZt2sQPP/zAyZMn82x9hJCQENq3b49Wq82R28ILIV4mI10Dpdfr8fHxYdq0aXm6II2bmxvNmjVjyZIlebZPIQoTGekaqNWrV7Nu3TqOHj2a56uAhYWF0aJFi6zbtAshco6UrgFKS0vDxcWF9evX06xZM1Uy9OvXDxcXFzw9PVXZvxAFlZSuAfL392fHjh38+uuvqmWIjIykcePGREZGUrx4cdVyCFHQSOkaGJ1OR9WqVdm+fTsNGjRQNct///tfHBwcmDZtmqo5hChIpHQNzOLFizl8+DC7d+9WOwp37tyhXr163Lhxg5IlS6odR4gCQUrXgKSkpODk5MSBAweoXbu22nEAGDlyJDY2NsyZM0ftKEIUCFK6BmTu3LmcPXuWgIAAtaNkuX//Pu+//z5hYWGULVtW7ThC5HtSugYiOTkZjUbD0aNHcXV1VTvOS8aOHYuiKCxatEjtKELke1K6BmL69Olcv36dDRs2qB3lFQ8ePMDV1ZXLly9ToUIFteMIka9J6RqAxMREqlatyqlTp6hataracf7WxIkTefLkCcuWLVM7ihD5mpSuAfDy8uL+/fusXLlS7Sj/KC4uDhcXF86fP0+VKlXUjiNEviWlq7I/yuzChQtUrlxZ7Tj/ysvLi6ioKFavXq12FCHyLSldlU2cOJGkpCSWLl2qdpTX+mMa5OTJkzg7O6sdR4h8SUpXRX8coAoJCcHR0VHtOG9kxowZhIWFsXHjRrWjCJEvSemqaOzYsQAsXLhQ1RzZ8cepbUeOHKFmzZpqxxEi35HSVUl0dDRubm6Ehobmu4sO5s6dy5kzZ9i2bZvaUYTId6R0VTJixAhsbW2ZPXu22lGyLSUlBY1Gw759+6hTp47acYTIV6R0VVAQFpJZvHgxgYGB7NmzR+0oQuQrUroq+O9//0v58uXx8/NTO8pb+2MJym3bttGwYUO14wiRb0jp5gVFgf+75U5kZCRNmjQhMjISOzs7dXO9I39/f7Zv386h/fvB1FTtOELkC1K6uS0z88V/jY0hM5N+AwZQrVo1pk6dqm6uHJCWksLacuXo2KYN5YcPh9at1Y4khMGTuwHnptWrwdERvL0BCA0NJTAwkDFjxqgcLAcoCubjx9OqRg1WXbuGMns2LFkCqalqJxPCoEnp5panT+GXX2DiRNi3D7RafPz8GP/11xQtWlTtdO8uORkuX6bS/v1sUBQutWoFERFgQGsBC2GIZHohN927BxUrwqRJPA4JwTUkBK1Wi5WVldrJckafPtC4MZtKlGDFwoUcGTECo3PnwMsL8tm5x0LkFRnp5qaKFV/8d+xYHgUH82O3bi8KNyND3Vw5pXt3uHwZj48+IjYlhZNJSWBhAb//rnYyIQyWlG4eOHvvHquALlevvnjAxATS01XNlCOaNYOSJTFZvx5fX1/GrFuHcu4cPH+udjIhDJaUbh7wmjqVKrNmYVymDIwZA6NGwaVLasd6d+XKQdeucOAA3fV6yup0PEpOltPHhPgXUrq57MSJE9yIjGSQhwfExsKmTVC1KjRooHa0nNGkCUyejPGvv/LzkyesePSIzA8+UDuVEAZLDqTlIkVRaNmyJQMHDmRgXBxER8Ps2S/mPQua9HQUoPGHHzJ27Fh69+6tdiIhDJKUbi46cuQII0eOJDQ0FFNj4xcXSBRwgYGBjBo1imvXrmEq0wxCvKLgt4BKFEXB09MTb2/vF+VTCAoXoHXr1pQuXVoWORfiHxSOJlDBgQMHSEpKwsPDQ+0oecrIyIjp06fj6+tLekE4Q0OIHCalmwsURcHLywtfX19MTEzUjpPnPvroI5ycnOQGlkL8DSndXPDLL7+QmZlJ9+7d1Y6immnTpjF9+nRSZS0GIV4ipZvDMjMz8fT0xM/PD+NCMo/7dxo1akStWrVYvny52lGEMCiFtxVySUBAANbW1nTq1EntKKrz8/Pju+++IyUlRe0oQhgMKd0cpNfr8fb2Ztq0aRj936LlhVndunVp3LgxS5cuVTuKEAZDztPNQevWrWPFihUEBQVJ6f6fa9eu0bp1a7RaLTY2NmrHEUJ1Uro5JD09nWrVqrFq1SqaN2+udhyD0qdPH2rWrMmUKVPUjiKE6qR0c8jy5cvZunUrgYGBakcxODdu3KBZs2YF4r5wQrwrKd0ckJqaStWqVdm6dSuNGjVSO45BGjhwIJUqVcLX11ftKEKoSko3B/zwww8cPHiQvXv3qh3FYN26dYsGDRpw48YNSpQooXYcIVQjpfuOnj9/jkajYc+ePdStW1ftOAZt2LBh2NvbM3PmTLWjCKEaKd139L///Y8TJ06wY8cOtaMYvKioKGrXrk1YWBhlypRRO44QqpDSfQdPnz5Fo9EQGBjI+++/r3acfGH06NGYmpryv//9T+0oQqhCSvcdzJw5kytXrrB582a1o+Qbv//+O66urly9epXy5curHUeIPCel+5aePHmCRqPhxIkTuLi4qB0nX/nmm29ISUlhyZIlakcRIs9J6b4lHx8f7ty5w5o1a9SOku88evSIatWqcfHiRSpVqqR2HCHylJTuW4iPj8fFxYWzZ8/y3nvvqR0nX5o6dSoPHjxgxYoVakcRIk9J6b6FyZMnk5CQgL+/v9pR8q3Hjx9TtWpVgoOD0Wg0ascRIs9I6WZTbGws1atX5/Lly1SoUEHtOPman58fWq2WdevWqR1FiDwjpZtNX3/9NXq9nsWLF6sdJd9LSkpCo9EQFBRE9erV1Y4jRJ6Q0s2GmJgY3n//fa5du0a5cuXUjlMgzJ49m4sXL/Lzzz+rHUWIPCGlmw1ffvkllpaWzJs3T+0oBcazZ8/QaDQcPHgQNzc3teMIkeukdN/Q3bt3qVu3LtevX6dUqVJqxylQFi5cyPHjx9m1a5faUYTIdVK6b2jIkCGULl2aGTNmqB2lwNHpdGg0Gnbt2sUHH3ygdhwhcpWU7hvQarU0atSIiIgI7O3t1Y5TIC1dupTdu3dz4MABtaMIkaukdN/AgAED0Gg0eHl5qR2lwEpLS8PZ2ZlNmzbRpEkTteMIkWukdF8jPDyc5s2bo9VqsbW1VTtOgbZq1So2btzIkSNH1I4iRK6RW7C/ho+PD+PGjZPCzQMDBgzg3r17HDt2TO0oQuQaGen+iytXrtCuXTu0Wi3W1tZqxykUNmzYwLJly/jtt9/kNvaiQJKR7r/w9vZmwoQJUrh56NNPPyUhIYFff/1V7ShC5AoZ6f6D8+fP061bNyIjIylSpIjacQqVgIAA5syZw9mzZ2W0KwocGen+RYsWLUhOTsbLy4spU6ZI4aqgR48epKens2fPHgIDA1m+fLnakYTIMVK6f3Ht2jV+++03wsLCGDhwIM+fP1c7UqFjbGyMn58fnp6eREREcPHiRbUjCZFjpHT/IiMjg1mzZjFixAjc3d3lBooqaNKkCdHR0ZibmxMSEkJGRobakYTIMTKn+xfW1tZZB84mTpzIV199hbGx/GzKSxEREfTu3ZsiRYqg1Wrp3LkzK1euVDuWEDlC2uQvdDodRkZG7Nu3j3HjxknhqsDZ2ZnTp0/zwQcfEBsbS2RkpNqRhMgxpmoHMDSjR4/Gy8uL4sWLqx2lULOwsGDRokVoNBoePXqkdhwhcoxMLwghRB4q8CPdtLQ0YmNjiY2NJT4+nszMzL99nrGxMSVKlKB06dKULl0ac3PzPE4q/omiKMTFxXHz5k2ePHnyyteLFCnCe++9h4ODg0wHCYNX4Eo3Ojqa77//nt9++43w8HCePn1K8eLFKVGiBHZ2dpiYmPzt6zIyMkhMTCQ+Pp7Hjx9jY2ND9erV+fDDDxk1ahSOjo55/E4Kt4cPH+Lt7c2JEye4c+cOZmZmODo6Ymtr+8oFEykpKdy/f5+kpCQqVKiAm5sbvr6+1KhRQ6X0QvyzAjW9kJ6eTu3atalZsybNmzenatWq2NnZZXv0k5mZSWJiIpGRkQQFBXHt2jUuX76MmZlZLiUXf5aRkUGDBg2oWrUqH3/8MeXLl6do0aKvfZ1Op+P+/fsEBwezYcMGwsLCKFmyZB4kFuLNFaiR7uXLl0lPT+ebb755p8tHjY2Nsbe3p2HDhjRo0IA+ffoQEhIidzXII+Hh4cTFxeHv75+t76OlpSVOTk44OTlx7do19u/fz4ABA3IxqRDZV6AmwMLDw6latWqOXq9vZGSERqMhPDw8x7Yp/l1OfB+dnJwICwvLwVRC5IwCVbphYWFUrFgxx7dbqVIl+Quch3Li+1i5cmVCQ0NzKJEQOadAle7169epVKlSjm+3UqVKXL9+Pce3K/7ejRs3cqR0IyIiciiREDmnQM3ppqamYmlpmfXnZcuWYWtrS58+fQBYsmQJ9vb2pKenc/jwYdLS0mjZsiXDhg3j+fPnTJo0idjYWDIyMhg8eDBt27YFXswV6nQ6Vd5TYfTX72NMTAyjR4+mdu3aXLlyhVKlSjF//nzu3r3LzJkz0el0ODo64uXllXWHD0tLS1JTU9V6C0L8owI10v2rLl26sG/fPuDFGQmHDh2iRIkSREVFsXbtWjZt2kR4eDgXL17k1KlTlCpVis2bN7N161a5OaKBiYqKolevXmzdupWiRYty9OhRvL29GTVqFFu2bEGj0cgSkCJfKNCl6+DgQLFixbh+/TrBwcG4uLgQFhZGcHAwffv2pV+/fty5c4d79+6h0Wg4c+YMixcv5tKlS9jY2KgdX/yJg4MDLi4uAFSrVo3o6GiSk5OpV68eAJ07d5YlIEW+UKCmF+DF1Ut/1q1bN/bu3Ut8fDxdunTh3LlzDBw4kB49erzy2g0bNnDy5EmWLl1K/fr1GTJkyN9uU+S+v37mfz5H2sTEhOTk5LyOJESOKFAj3VKlSpGQkPDSYy1btuTUqVOEhYXRuHFjGjduzO7du0lJSQEgNjaWhIQEHj16hKWlJR07dqR///4vHTiLj4+ndOnSefpeCrOSJUu+8n38KxsbG2xtbbl06RIA+/bto27dullfj4uLkwsjhEEqUCPdGjVqvHKakJmZGR988AFFixbFxMSERo0acfv2bQYNGgSAlZUV06ZNIyoqikWLFmFsbIypqSmTJk3K2sadO3eoVatWnr6XwszV1ZUTJ0689nk+Pj5ZB9LKly+Pt7d31tfu3LkjlwELg1TgSnf//v0vPZaZmcm1a9eYNWtW1mOffvopn3766UvPc3R0pHHjxn+73bt37+Lh4ZHzgcXfql69Ohs2bMj6s4ODA1u3bs36c//+/bN+v2bNmr/dxp07d3B1dc21jEK8rQI1veDm5kZoaChJSUkA3Lp1i+7du1O/fv23Pu/zyZMnhIWFyUg3D73//vtERES8dorhn+j1es6dO/fSdIMQhqJALXgDMH78eNasWUPdunVxcnKiRIkS2NvbZ2uVsYSEBOLj47l58yYXL15k0KBBzJ07N4/fSeHm4+ODv78/TZo0wcHBAUdHx39cZezZs2fcv38/69fFixepXr06u3btklu4C4NT4EoXXoxwT506RWhoKA8ePODBgwfExsYSFxf3r+vplixZktKlS1O2bFnKli2Lq6srTZo04b333svjdyAALl26xOnTp9Fqtdy8efO16+k6OTmh0WioVasWrVq1ksIVBqlAlq4QQhiqAjWnK4QQhk5KVwgh8pCUrhBC5KH/D31LBfBduzqTAAAAAElFTkSuQmCC\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************************\n",
      "[3, 2, 3, 1, 3, 2, 3, 1, 3, 2, 3, 1, 3, 2, 3, 3, 3, 3, 3, 1, 3, 2, 3, 3]\n",
      "[['1' '1' '1' '1' '3']\n",
      " ['1' '1' '1' '2' '2']\n",
      " ['1' '1' '2' '1' '3']\n",
      " ['1' '1' '2' '2' '1']\n",
      " ['1' '2' '1' '1' '3']\n",
      " ['1' '2' '1' '2' '2']\n",
      " ['1' '2' '2' '1' '3']\n",
      " ['1' '2' '2' '2' '1']\n",
      " ['2' '1' '1' '1' '3']\n",
      " ['2' '1' '1' '2' '2']\n",
      " ['2' '1' '2' '1' '3']\n",
      " ['2' '1' '2' '2' '1']\n",
      " ['2' '2' '1' '1' '3']\n",
      " ['2' '2' '1' '2' '2']\n",
      " ['2' '2' '2' '1' '3']\n",
      " ['2' '2' '2' '2' '3']\n",
      " ['3' '1' '1' '1' '3']\n",
      " ['3' '1' '1' '2' '3']\n",
      " ['3' '1' '2' '1' '3']\n",
      " ['3' '1' '2' '2' '1']\n",
      " ['3' '2' '1' '1' '3']\n",
      " ['3' '2' '1' '2' '2']\n",
      " ['3' '2' '2' '1' '3']\n",
      " ['3' '2' '2' '2' '3']] <class 'numpy.ndarray'> ['age', 'prescript', 'astigmatic', 'tearRate'] <class 'list'>\n",
      "[2, 1, 2, 1, 2, 1, 2, 3, 3, 1, 2, 3]\n",
      "[['1' '1' '1' '2']\n",
      " ['1' '1' '2' '1']\n",
      " ['1' '2' '1' '2']\n",
      " ['1' '2' '2' '1']\n",
      " ['2' '1' '1' '2']\n",
      " ['2' '1' '2' '1']\n",
      " ['2' '2' '1' '2']\n",
      " ['2' '2' '2' '3']\n",
      " ['3' '1' '1' '3']\n",
      " ['3' '1' '2' '1']\n",
      " ['3' '2' '1' '2']\n",
      " ['3' '2' '2' '3']] <class 'numpy.ndarray'> ['age', 'prescript', 'astigmatic'] <class 'list'>\n",
      "[1, 1, 1, 3, 1, 3]\n",
      "[['1' '1' '1']\n",
      " ['1' '2' '1']\n",
      " ['2' '1' '1']\n",
      " ['2' '2' '3']\n",
      " ['3' '1' '1']\n",
      " ['3' '2' '3']] <class 'numpy.ndarray'> ['age', 'prescript'] <class 'list'>\n",
      "[1, 3, 3]\n",
      "[['1' '1']\n",
      " ['2' '3']\n",
      " ['3' '3']] <class 'numpy.ndarray'> ['age'] <class 'list'>\n",
      "[3]\n",
      "[3]\n",
      "[1]\n",
      "[1, 1, 1]\n",
      "[2, 2, 2, 2, 3, 2]\n",
      "[['1' '1' '2']\n",
      " ['1' '2' '2']\n",
      " ['2' '1' '2']\n",
      " ['2' '2' '2']\n",
      " ['3' '1' '3']\n",
      " ['3' '2' '2']] <class 'numpy.ndarray'> ['age', 'prescript'] <class 'list'>\n",
      "[2, 2]\n",
      "[3, 2]\n",
      "[['1' '3']\n",
      " ['2' '2']] <class 'numpy.ndarray'> ['prescript'] <class 'list'>\n",
      "[2]\n",
      "[3]\n",
      "[2, 2]\n",
      "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "{'tearRate': {'2': {'astigmatic': {'2': {'prescript': {'2': {'age': {'2': 3, '3': 3, '1': 1}}, '1': 1}}, '1': {'age': {'2': 2, '3': {'prescript': {'2': 2, '1': 3}}, '1': 2}}}}, '1': 3}}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    决策树(手撕代码:ID3算法实现)\n",
    "        - 决策树采用的是信息熵或者Gini系数来作为分类标准(手动计算时使用)\n",
    "            - 信息熵：是度量样本集合纯度的常用指标，该值越大，表示该集合纯度越低(混乱度越大)；该值越小，表示该集合纯度越高(混乱度越低)\n",
    "                                        |y|\n",
    "                - 信息熵公式 Ent(D) =  -   ∑  Pₖlog₂Pₖ\n",
    "                                        k=1\n",
    "                    - D                         总样本空间\n",
    "                    - Pₖ                        子样本空间占总样本空间比值\n",
    "                    - y                         结果集\n",
    "            - Gini系数（CART算法使用） 反映了从数据集中随机选择两个样本，类别不一致的概率。因此，基尼系数越小，数据集的纯度越高。CART决策树\n",
    "                    (Classification And Regression Tree)使用基尼系数进行划分属性选择。\n",
    "                - Gini系数公式\n",
    "                                 K                    K\n",
    "                    - Gini(p) =  ∑   Pₖ(1 - Pₖ) = 1 -  ∑ Pₖ²\n",
    "                                K=1                  K=1\n",
    "                    - 数据集下根据属性a划分的基尼系数\n",
    "                                           v    Dᵛ\n",
    "                        Gini_index(D,a) =  ∑   ———— Gini(Dᵛ)\n",
    "                                          v=1   D\n",
    "            - 信息增益（ID3算法使用）  决策树根据属性进行判断，将具有相同属性的样本划分到相同的节点下面，此时，样本较划分之前具有更高的纯度，信息熵值有所下\n",
    "                       降。此时，使用划分前的信息熵减去划分后的信息熵就可以得到决策树在划分前后获得的信息增益。\n",
    "                                        v   Dᵛ\n",
    "                - Gain(D,a) = Ent(D) -  ∑  ————  Ent(Dᵛ)\n",
    "                                       v=1  D\n",
    "                    - D                                 样本集合\n",
    "                    - a                                 特征\n",
    "                    - v                                 表示特征a可能的取值种类\n",
    "                    - Dᵛ/D                              表示经过特征a划分之后每一个类别的权重。\n",
    "                    - Ent(D)                            信息熵\n",
    "                    - Ent(Dᵛ)                           子样本信息熵\n",
    "\n",
    "            - 增益率（C4.5算法使用） 增益率不直接使用信息增益，而是使用信息增益与信息熵的比值作为衡量特征优劣的标准，C4.5算法就是使用增益率作为标准选择最优\n",
    "                     划分属性。\n",
    "                                        Gain(D,a)\n",
    "                    - Gainᵣatio(D,a) =   ————————\n",
    "                                          IV(a)\n",
    "                                        v   Dᵛ          Dᵛ\n",
    "                        - IV(a) =    -  ∑  ————  log₂  ————                 信息熵\n",
    "                                       v=1  D           D\n",
    "                            - Dᵛ/D                              表示经过特征a划分之后每一个类别的权重。\n",
    "\n",
    "\"\"\"\n",
    "import sklearn.datasets as sd\n",
    "# 打乱数据集用\n",
    "import sklearn.utils as su\n",
    "import sklearn.ensemble as se\n",
    "import numpy as np\n",
    "\n",
    "boston = sd.load_iris()\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x, y = su.shuffle(boston.data, boston.target, random_state=7)\n",
    "labels = boston.feature_names\n",
    "result_target_names = boston.target_names\n",
    "\n",
    "\n",
    "#如果用下面自定义数据集则需注释掉本代码\n",
    "target_names_dict={}\n",
    "for index,label in enumerate(result_target_names):\n",
    "    target_names_dict[float(index)] = label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#划分训练集和测试集\n",
    "train_size= int(len(x)*0.8)\n",
    "train_x,train_y,test_x,test_y = x[:train_size],y[:train_size].reshape(-1,1),x[train_size:],y[train_size:].reshape(-1,1)\n",
    "\n",
    "\n",
    "train_dataSet = np.hstack((train_x,train_y))\n",
    "test_dataSet = np.hstack((test_x,test_y))\n",
    "# data_labels =list(labels)\n",
    "\n",
    "#可以用这个手写的数据集看图\n",
    "dataSet=[\n",
    "                [1,1,'yes'],\n",
    "                [1,1,'yes'],\n",
    "                [1,0,'no'],\n",
    "                [0,1,'no'],\n",
    "                [0,1,'no']\n",
    "             ]\n",
    "train_dataSet =np.array(dataSet)\n",
    "\n",
    "target_names_dict= {\"yes\":\"no surfacing\",\"no\":\"flippers\"}\n",
    "labels=['no surfacing','flippers']\n",
    "\n",
    "\n",
    "\n",
    "# data_labels.append(\"classification results\")\n",
    "# print(data_labels)\n",
    "# df = pd.DataFrame(train_dataSet,columns=data_labels)\n",
    "# df.to_csv(\"鸢尾花数据集.csv\")\n",
    "\n",
    "\n",
    "\n",
    "#计算信息熵\n",
    "def calcInfoEntropy(dataSet):\n",
    "    dataSet_length = len(dataSet)\n",
    "    sub_sample_set = {}\n",
    "    '''\n",
    "                            |y|\n",
    "      信息熵公式 Ent(D) =  -   ∑  Pₖlog₂Pₖ\n",
    "                            k=1\n",
    "    '''\n",
    "    # 通过结果集划分子样本空间\n",
    "    for row in dataSet:\n",
    "        if row[-1] not in sub_sample_set:\n",
    "            sub_sample_set[row[-1]] = 0\n",
    "        sub_sample_set[row[-1]]+=1\n",
    "    # print(sub_sample_set)\n",
    "    #计算信息熵\n",
    "    infoEntropy = 0.0\n",
    "    for sub_sample in sub_sample_set:\n",
    "        # 计算 Pₖ\n",
    "        pk = sub_sample_set[sub_sample] / float(dataSet_length)\n",
    "        # 计算香浓熵\n",
    "\n",
    "        infoEntropy += pk * np.log2(pk)\n",
    "    #返回计算好的信息熵\n",
    "    return -infoEntropy\n",
    "\n",
    "# 划分数据集\n",
    "def split_dataSet(dataSet,column_index,unique_value):\n",
    "    # print(dataSet,column_index,unique_value)\n",
    "    new_dataSet = []\n",
    "    for row in dataSet:\n",
    "        if row[column_index]  == unique_value:\n",
    "            sub_dataSet = list(row[:column_index])\n",
    "            sub_dataSet.extend(row[column_index+1:])\n",
    "            new_dataSet.append(sub_dataSet)\n",
    "    return np.array(new_dataSet)\n",
    "\n",
    "#选择最优特征\n",
    "def choose_best_feature_to_split(dataSet):\n",
    "    #计算未切割数据集信息熵\n",
    "    infoEntropy = calcInfoEntropy(dataSet)\n",
    "    #计算信息增益\n",
    "    '''\n",
    "                                    v   Dᵛ\n",
    "            - Gain(D,a) = Ent(D) -  ∑  ————  Ent(Dᵛ)    信息增益越大,信息纯度越小(寻找纯度小的进行划分)\n",
    "                                   v=1  D\n",
    "\n",
    "    '''\n",
    "    bestInfoGain = 0.0\n",
    "    bestFeatureIndex = -1\n",
    "    #单个样本长度(不包括最后一列结果集数据)\n",
    "\n",
    "\n",
    "\n",
    "    trainset_row_length = len(dataSet[0])-1\n",
    "\n",
    "\n",
    "    for column_index in range(trainset_row_length):\n",
    "        # 获取样本集中特征列数据\n",
    "        column_value = [row[column_index] for row in dataSet]\n",
    "        # 获得特征列唯一值集合\n",
    "        sub_sample_set = set(column_value)\n",
    "\n",
    "\n",
    "\n",
    "        newInfoGain = 0.0\n",
    "        '''\n",
    "                   v   Dᵛ\n",
    "           -  计算  ∑  ————  Ent(Dᵛ)\n",
    "                  v=1  D\n",
    "        '''\n",
    "        for unique_value in sub_sample_set:\n",
    "            subDataSet = split_dataSet(dataSet,column_index,unique_value)   #子表构成的数据集\n",
    "            # print(newDataSet)\n",
    "            #计算信息增益\n",
    "            # Ent(Dᵛ)\n",
    "            Entdv = calcInfoEntropy(subDataSet)   #子表信息熵\n",
    "\n",
    "\n",
    "            newInfoGain += len(subDataSet) / float(len(dataSet)) * Entdv\n",
    "\n",
    "        #获得信息熵\n",
    "        infoGain = infoEntropy - newInfoGain\n",
    "        if infoGain > bestInfoGain:\n",
    "            bestInfoGain = infoGain\n",
    "            bestFeatureIndex = column_index\n",
    "\n",
    "    return bestFeatureIndex\n",
    "\n",
    "import operator\n",
    "\n",
    "def majorityCnt(classList):\n",
    "    class_count = {}\n",
    "    for vote in classList:\n",
    "        if vote not in class_count:\n",
    "            class_count[vote]  = 0\n",
    "        class_count[vote] += 1\n",
    "    sorted_class_count = sorted(class_count.items(),key=operator.itemgetter(1),reverse=True)\n",
    "    return sorted_class_count[0][0]\n",
    "\n",
    "#创建决策树\n",
    "def create_decision_tree(dataSet,labels):\n",
    "    #获取最后一列类别标签\n",
    "    class_list = [row[-1] for row in dataSet]\n",
    "    try:\n",
    "        class_list = list(map(int,class_list))\n",
    "        class_list = [target_names_dict[n] for n in class_list]\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    # 用自定义数据集则需注释掉本行代码\n",
    "    # class_list = [target_names_dict[n] for n in class_list]\n",
    "\n",
    "    print(class_list)\n",
    "\n",
    "    # print(class_list)\n",
    "    # 类别完全相同则停止划分\n",
    "    if class_list.count(class_list[0]) == len(class_list):\n",
    "        return class_list[0]\n",
    "    # 遍历完所有特征时返回出现次数最多的分类信息\n",
    "    if len(dataSet[0]) == 1:\n",
    "        return majorityCnt(class_list)\n",
    "    bestFeatureIndex = choose_best_feature_to_split(dataSet)\n",
    "    # 拿到要切分的最优特征列名称\n",
    "    bestFeatureLabel = labels[bestFeatureIndex]\n",
    "    myTree = {bestFeatureLabel:{}}\n",
    "    print(dataSet,type(dataSet),labels,type(labels))\n",
    "\n",
    "    # 删除最优特征列标签\n",
    "    del(labels[bestFeatureIndex])\n",
    "    # 得到列表包含的所有特征值\n",
    "    feature_cloumn_value = [row[bestFeatureIndex] for row in dataSet]\n",
    "    sub_sample_set = set(feature_cloumn_value)\n",
    "    for unique_value in sub_sample_set:\n",
    "        # 删除最优特征列标签后剩下的标签\n",
    "        subLabels = labels[:]\n",
    "\n",
    "        myTree[bestFeatureLabel][unique_value] = create_decision_tree(split_dataSet(dataSet,bestFeatureIndex,unique_value),subLabels)\n",
    "    return myTree\n",
    "\n",
    "\n",
    "myTree = create_decision_tree(train_dataSet,labels)\n",
    "\n",
    "print(myTree)\n",
    "\n",
    "\n",
    "\n",
    "# myTree =create_decision_tree(test_dataSet,labels)\n",
    "# print(myTree)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#绘制树形图\n",
    "import matplotlib.pyplot as plt\n",
    "decision_node = dict(boxstyle=\"sawtooth\",fc=\"0.8\")\n",
    "leaf_node = dict(boxstyle=\"round4\",fc=\"0.8\")\n",
    "arrow_args = dict(arrowstyle=\"<-\")\n",
    "#获取树的叶子结点个数（确定图的宽度）\n",
    "def get_leaf_num(tree):\n",
    "    leaf_num = 0\n",
    "    first_key = list(tree.keys())[0]\n",
    "    next_dict = tree[first_key]\n",
    "    for key in next_dict.keys():\n",
    "        if type(next_dict[key]).__name__==\"dict\":\n",
    "            leaf_num +=get_leaf_num(next_dict[key])\n",
    "        else:\n",
    "            leaf_num +=1\n",
    "    return leaf_num\n",
    "#获取数的深度（确定图的高度）\n",
    "def get_tree_depth(tree):\n",
    "    depth = 0\n",
    "    first_key = list(tree.keys())[0]\n",
    "    next_dict = tree[first_key]\n",
    "    for key in next_dict.keys():\n",
    "        if type(next_dict[key]).__name__ == \"dict\":\n",
    "            thisdepth = 1+ get_tree_depth(next_dict[key])\n",
    "        else:\n",
    "            thisdepth = 1\n",
    "        if thisdepth>depth: depth = thisdepth\n",
    "    return depth\n",
    "\n",
    "def plotNode(nodeTxt, centerPt, parentPt, nodeType):\n",
    "    createPlot.ax1.annotate(nodeTxt, xy=parentPt, xycoords='axes fraction',\n",
    "                            xytext=centerPt, textcoords='axes fraction',\n",
    "                            va=\"center\", ha=\"center\", bbox=nodeType, arrowprops=arrow_args)\n",
    "\n",
    "def retruieveTree(i):\n",
    "    listOfTrees = [\n",
    "                {\"no surfacing\":{0:\"no\",1:{\"flippers\":{0:\"no\",1:\"yes\"}}}},\n",
    "                {\"no surfacing\":{0:\"no\",1:{\"flippers\":{0:{\"head\":{0:'no',1:\"yes\"}},1:\"no\"}}}}\n",
    "                   ]\n",
    "    return listOfTrees[i]\n",
    "#在父子节点间填充文本信息\n",
    "def plotMidText(cntrPt, parentPt, txtString):\n",
    "    xMid = (parentPt[0] - cntrPt[0]) / 2.0 + cntrPt[0]\n",
    "    yMid = (parentPt[1] - cntrPt[1]) / 2.0 + cntrPt[1]\n",
    "    # createPlot.ax1.text(xMid, yMid, txtString, va=\"center\", ha=\"center\",color =\"red\", rotation=30)\n",
    "    createPlot.ax1.text(xMid, yMid, txtString, va=\"center\", ha=\"center\",color =\"red\", rotation=30)\n",
    "\n",
    "def plotTree(myTree, parentPt, nodeTxt):\n",
    "    numLeafs = get_leaf_num(myTree)\n",
    "    depth = get_tree_depth(myTree)\n",
    "    firstStr = list(myTree.keys())[0]\n",
    "    cntrPt = (plotTree.xOff + (1.0 + float(numLeafs)) / 2.0 / plotTree.totalW, plotTree.yOff)\n",
    "    plotMidText(cntrPt, parentPt, nodeTxt)\n",
    "    plotNode(firstStr, cntrPt, parentPt, decision_node)\n",
    "    secondDict = myTree[firstStr]\n",
    "    plotTree.yOff = plotTree.yOff - 1.0 / plotTree.totalD\n",
    "    for key in secondDict.keys():\n",
    "        if type(secondDict[\n",
    "                    key]).__name__ == 'dict':\n",
    "            plotTree(secondDict[key], cntrPt, str(key))\n",
    "        else:\n",
    "            plotTree.xOff = plotTree.xOff + 1.0 / plotTree.totalW\n",
    "            plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leaf_node)\n",
    "            plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key))\n",
    "    plotTree.yOff = plotTree.yOff + 1.0 / plotTree.totalD\n",
    "\n",
    "\n",
    "\n",
    "def createPlot(inTree):\n",
    "    fig = plt.figure(1, facecolor='white')\n",
    "    fig.clf()\n",
    "    axprops = dict(xticks=[], yticks=[])\n",
    "    createPlot.ax1 = plt.subplot(111, frameon=False, **axprops)\n",
    "    plotTree.totalW = float(get_leaf_num(inTree))\n",
    "    plotTree.totalD = float(get_tree_depth(inTree))\n",
    "    plotTree.xOff = -0.5 / plotTree.totalW\n",
    "    plotTree.yOff = 1.0\n",
    "    plotTree(inTree, (0.5, 1.0), '')\n",
    "    plt.show()\n",
    "createPlot(myTree)\n",
    "\n",
    "#创建决策树分类器\n",
    "def classify(inputTree,featureLabels:\"特征标签\",testVec):\n",
    "\n",
    "    firstStr = list(inputTree.keys())[0]\n",
    "\n",
    "    secondDict = inputTree[firstStr]\n",
    "\n",
    "    featureIndex = featureLabels.index(firstStr)   #将标签转化为索引\n",
    "    # print(secondDict)\n",
    "    classLabel = []\n",
    "    for key in secondDict.keys():\n",
    "        if testVec[featureIndex] == key:\n",
    "            if type(secondDict[key]).__name__ == \"dict\":\n",
    "                classLabel = classify(secondDict[key],featureLabels,testVec)\n",
    "            else:\n",
    "                classLabel = secondDict[key]\n",
    "    return classLabel\n",
    "\n",
    "print(\"*\" * 70)\n",
    "# 预测隐形眼镜\n",
    "\n",
    "with open(\"sklearn_data/lenses.data\") as f:\n",
    "    lenses = [inst.strip(\"\\n\").split(\"  \") for inst in f.readlines()]\n",
    "    lensesLabels = [\"age\",\"prescript\",\"astigmatic\",\"tearRate\"]\n",
    "\n",
    "    lensesTree = create_decision_tree(np.array(lenses),lensesLabels)\n",
    "    print(lensesTree)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    决策树(手撕代码:CART算法实现)\n",
    "\"\"\"\n",
    "import sklearn.datasets as sd\n",
    "# 打乱数据集用\n",
    "import sklearn.utils as su\n",
    "import sklearn.ensemble as se\n",
    "import numpy as np\n",
    "#加载鸢尾花数据集\n",
    "boston = sd.load_iris()\n",
    "x, y = su.shuffle(boston.data, boston.target, random_state=7)\n",
    "labels = boston.feature_names\n",
    "result_target_names = boston.target_names\n",
    "\n",
    "\n",
    "#如果用下面自定义数据集则需注释掉本代码\n",
    "target_names_dict={}\n",
    "for index,label in enumerate(result_target_names):\n",
    "    target_names_dict[float(index)] = label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#划分训练集和测试集\n",
    "train_size= int(len(x)*0.8)\n",
    "train_x,train_y,test_x,test_y = x[:train_size],y[:train_size].reshape(-1,1),x[train_size:],y[train_size:].reshape(-1,1)\n",
    "\n",
    "\n",
    "train_dataSet = np.hstack((train_x,train_y))\n",
    "test_dataSet = np.hstack((test_x,test_y))\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}